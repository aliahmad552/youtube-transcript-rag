{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwIPfRtDnK9n",
        "outputId": "538e7d57-8b5c-4a15-e5aa-db66065a141e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.0/485.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q youtube-transcript-api langchain-community langchain-huggingface faiss-cpu tiktoken python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hazc86mIn2Md"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound, VideoUnavailable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aML7HnXooLno"
      },
      "source": [
        "# Step 1a: Indexing/(Doucment Ingestion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wasJ6GDjoSJf"
      },
      "outputs": [],
      "source": [
        "video_id = \"ZhAz268Hdpw\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xXFgOiKo1Lk",
        "outputId": "d7020893-fe4e-4699-81b4-4881a8adbc5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chad GPT is powered by a model calledGPT which is based on a deep learningarchitecture called TransformersTransformers is the reason behind modernday AI boom as an AI Enthusiast when youstart learning Transformers you willcome across this complex diagram whichwill start giving you a headacheimmediately my goal for today's video isto explain you Transformers in a mostsimplified and intuitive manner we needto cover many different topics so thisis going to be a long video attentionand patience is all I need from youtoday when you type in a sentence inGmail it tries to predict next word ornext set of words this is possiblebecause of a machine learning modelcalled language model Google for examplehas this popular language model calledbird which is powering hundreds of AIapplications throughout the world GPTwhich is a model behind chat GPT is alarge language model the reason it iscalled large language model is becauseit has billions of parameters it is muchmore capable and Powerful compared tobird and it is trained on humongousamount of data fundamentally though itis also doing the same thing which iswhen you type in a question in chat GPTit will predict the next word in thatsentence and then it will take theoriginal question and the next predictedword as an input and then predict thenext word and then the next word and soon in the end it produces is a completeanswer which almost sounds like a magicto summarize the goal of a languagemodel is to predict a next word in asentence now that we have understoodthis fundamental let's look into some ofthe topics which needs to be clarifiedbefore we dig into the actualarchitecture the first concept we needto understand is word embedding machinelearning models do not understand textthey understand numbers so we need torepresent text as numbers let's say youhave this word King you want tonumerically represent it how would youdo that well you can assign a fixednumber you can have a vocabulary and youcan assign just fixed static number butthat will not capture the meaning of itwhen you're building a language modelyou have to represent words in such away that they capture the meaning ofthat word one way to capture the meaningof this word King and represent itnumerically is to ask bunch of questionsfor example does this person hasAuthority yes one do they have a tail nohorse has a tail King doesn't have atail are they Rich yes gender minus oneis male one is female and so on what wejust did is we created this Vector listof numbers which is a vector torepresent this word King similarly wecan represent the word queen as well notonly that we can can represent bunch ofwords such as battle horse King and soon by asking set of questions okay sofor battle they don't have authority arethey an event yes do battle has tail nobattle is an event it it doesn't havetail and so on similarly horse do theyhave authority well we'll just say 01 ifit is King's horse maybe they have someAuthority or maybe they have authorityover their horse kids and so onsimilarly we can represent all thesewords in a numeric format and then wecan take the vector of this word Kingand maybe we can do some mathematicswith it we can say King minus man sohere I'm taking the vector of man rightwhich is uh this particular Vector pluswoman which is this particular vectorand when you do the math which is like 1- 2 + 2 will beZ and so on you get a vector which lookssimilar to Queen now this sounds like amagic we can do math with Words which isKing minus man plus woman is equal toQueen here King was represented in fiveDimensions when you look at the reallife for example Google's word to wackmodel it has 300 dimensions and what areall these questions by the way well weactually don't know this has beentrained through a neural network and wehave processed huge amount of text suchas all the Wikipedia articles all thebooks and text on internet to understandthe relationship between these words andthrough that neural network trainingback propagation we came up with thisVector the example that I gave for Kingwhere we asked these questions Authorityand so on that was just a madeup examplefor building intuition for wordembeddings in real life we do not knowwhat all these number means all we cansay is these numbers are the featuresfor this word and they capture themeaning of this word King let's say kingis a three-dimensional embedding if youhave to represent that in this 3Dembedding space you can represent itlike this where x axis has three likethis three number y AIS has this eightnumber z-axis has this two number and soon I use three dimensions because ashumans we can view only three dimensionswe can't possibly view this 300Dimension okay but mathematically those300 dimensions are possible models likeGPT uses an embedding Vector which iseven 12,000 Dimensions okay so it's avery rich High dimensional space that weare working with in threedimensionalspace you can have vectors for King kingand queen that looks something like thisand if you look at this Vector which isjoining king and queen you can think ofthat as a gender Direction the benefitof this gender Direction Vector is thatwhen you have another embedding forUncle you can add that gender Directionand get the embedding for word Auntsimilarly if you have father you can getmother if you have man you can uh getwoman and so on and that allows you todo this amazing math such as king minusQueen plus uncle is equal to Auntanother example is you can have countryto Capital City Direction Vector whichyou can use to add it in this embedingof Russia to get the embedding of Moscowyou can do Russia minus Moscow plusDelhi equal to India now the embeddingthat we're talking about are staticembeddings wtu and glow are two popularmodelswhich helps you get the static embeddingstatic means fixed embedding for allthese words you may ask how theseembeddings are generated well I alreadyanswered the question which is you traina neural network model on humongousamount of text Wikipedia books and so onto understand the relationship betweenthe words I'm not going to go into themathematical details of word to you canrefer to some other material on internetI have YouTube videos for that I'm notgoing to go into the math of that butjust think that uh the neural networktries to understand the relationshipbetween these words and creates thesestatic embeddings now the problem withstatic embedding is that you can have astatic embedding for this word track butbased on a sentence that track can memean different things right like hereI'm saying the train will run on thetrack and my package is late help metrack it so the meaning of track islittle different and when you havestatic embedding you get into thisproblem where you're not able torepresent this word properly based onthe context of this sentence you willsee same issue here for Dish you canhave a fixed embedding but in thissentence I'm talking about rice dish ifI had a cheese dish then the embeddingof dish should be a little differentbecause the meaning of that dish word islittle different when I say rice dishversus cheese dish when you are workingon predicting the next word for thissentence you can have words such asrisotto itly Mexican rice but when I sayI made an Indian rice dish call all of asudden the probabilities of my nextwords will change I will have words suchas idly Biryani ke if I add one moreadjective and say I made a sweet Indianrice dish in that case again it willchange I will not have Biryani as a nextword prediction I will probably have Kor Pongal to summarize to build anapplication like CH GPT just the staticword embeddings are not enough what youneed is contextual embedding let'sunderstand contextual embedding a bitmore in detail when you represent thisword dish in your embedding space it isaesthetic embedding when you say risethis maybe there is another Vector inthe same space which can accuratelydescribe rice dis or which can correctlycapture the meaning of word rice dishwhich can be Roto Biryani and so on thedirection from dish to rice dish we cancall it ress and when you add that ressVector to Dish what you get is theembedding for a rice dish there isanother vector or embedding for Indianrice dis and to go from Rice dis toIndian rice dis you need to probably addthis vector or a direction calledindianness and same thing for sweetIndian rice dish in order to generatecontextual embedding what we need to dois take the original static embeddingfor the word dish and have all theseother words influence that staticembedding or change that staticembedding so that it can capture themeaning of all these adjectives once youhave done that and once you have a acontextual embedding for dash it won'tbe hard to predict the next word whichis K look at this another sentence whereI'm saying D loves Dosa Dola and Milletand so on B loves pasta and so on theyboth went out for a dinner and here BNsaid bro we'll go to a restaurant thatyou like and after some time they werein Indian restaurant now the way youpredicted this word Indian was based onthis contsuch as D LS all these items which arepart of the Indian Cuisine also bin saidto D bro will go to a restaurant youlike if bin said here instead of you ifhe had said I this word will becomeItalian instead of Indian right alsoinstead of B if there was double herethen also this thing will become Italianso you can understand that this uhprediction Indian uh is influenced bynot just the few words which are priorto that word but it can be influenced bysome words which are far out in thatparagraph okay to summarize theobjective for this intelligent teachercat is to generate a contextualembedding and if you think about thisembedding space mathematically speakingwhat you're doing is taking the staticembedding for the word Dash and thenadding the embeddings for all thesevectors ress indianness all theseadjectives and getting your finalcontextual embedding let's now dig intothe Transformer architecture thearchitecture has two components encoderand decoder the purpose of encoder is totake the input sentence and generate thecontextual embedding for each of thewords or each of the tokens in thatsentence once the contextual embeddingis generated we feed that to a decoderhere and we try to predict the next wordso if you're working on the next wordprediction you will predict the nextword here for example it will be herewhen you talk about natural languageprocessing there are multiple tasks soone task is to predict the next word theother task would be to translate thesentence here I'm translating thesentence from English English to Hindiin that case you will still produce thecontextual embedding from encoder youfeed that to decoder and decoder willstart predicting the next word so hereit will start with this fixed starttoken and then it will uh produce theprobability of the next word so here theprobability of this word man is highestand here you can have the entirevocabulary for example in the case ofbird you have some 30,000 words soyou'll have all the words in yourlanguage and you will say okay what isthe highest probability of my next wordthen you put that word man into thisinput okay so there are two inputsactually one is the contextual embeddingwhich is coming uh from here from theencoder and the other one is whateveroutput you have produced so far from theprevious step you feed that okay as aninput here and then it will produce thenext word which is K once again youprovide key here and it produces banaiSo eventually it produces the entiretranslated sentence all right so that'sthe objective of your encoder anddecoder part whatever I talked about sofar I was referring to an inferencestage of uh neural networks whenever youhave these uh deep learning model youhave two stages one stage is the modelis not trained it's like a baby baby isnot trained yet and you train them rightyou send them to school you train themuh in your home at some point theybecome adult and they can figure thingsout on their own similarly a machinelearning model goes through a trainingphase and when it is ready it startsworking on the real world problem andthat is called inference so whatever Italked about for for predicting the nextword or translating sentence I wasreferring to inference stage throughoutthe discussion we'll be referring to twospecific models called bird and GPT ifyou look at this architecture that's ageneric architecture for a Transformermodel Transformer model is a generalconcept whereas bird and GPT arespecific model or specificimplementations based on Transformerarchitecture if you look at birdarchitecture it has only the encoderpart okay so only this part decoder partis missing so but will take the inputsentence it will produce the contextualembedding and that's it whereas GPT hasonly decoder so it still takes the inputit will produce the contextual embeddingand so on and then here it will predictthe next word I mean it sounds like ithas encoder decoder both butfundamentally the architecture lookslittle different for GPT but it is stillbased on the Transformer architecturethe way they're trained is you take allthe text from Wikipedia crawl text frominternet book Corpus and you train thesemodels when you have this article forexample and you are having this sentencedeveloping an advanced crude see if Igive you this sentence most likely youwill say spacecraft or vehicle as a nextword you'll not say banana right like soprobability of having banana as a nextword in this sentence is very lesswhereas these two words have a highprobability so we as humans have read somuch text so now we have learned thisart of predicting next word and samething goes on for B and GPT where theyunderstand the relationship between thewords the context in which they appearso let's say if B during the traininghas encountered so much text and everytime after this word crude if it hasseen this word spacecraft of or vehicleuh it would not have seen words likecrude chair or crude banana right thatkind of words usually when it is goingthrough training it will not see it soit will learn to predict the highprobability worse right so for word likebanana probability is going to be lowersame thing for this article when you gothrough this kind of sentences right SIengaging both alliances and hostilitiesand there will be many more artic onbattles and Warriors and everywhereafter alliances there will be eitherhostilities or negotiations there willnot be a word like chair so probabilityof that will be very very low now whenyou go through the training you aregoing through all these words right soall these words will form somethingcalled a vocabulary so for a Model Avocabulary will look something like thisnow there is a difference between a wordand a token so for example here playingis a word and one of the way to tokenizeis to have two token so one token isplay second token is ing okay so tokenwise there are two tokens but word isjust once but just for uh understandingpurpose just for easy explanation youcan think of word as tokens technicallythey're different but you can think ofwords as token only okay so let's sayyou have a vocabulary of all thesetokens let's say30,000 words what happens here is foreach of these words during the trainingit will create those static embeddingsso for word made or let's say for wordand seven is the index and let's saythis is the static embedding vector andthe dimension or the size see there is adot dot dot so what is the size of thiswell for bird it is 768 for GPT it's12,228 right so based on model thedimension of your embedding Vector canvary when you go through this trainingfor every token in your vocabulary youwill have this static embedding and thiswhole table is called Static embeddingMatrix during the training it will alsolearn few other things such as WQ WK WVand you are like what the hell this iswell we will talk about this later butfor now just remember when you trainthese models they are having this staticword embedding metrix which is thestatic embedding for every token in yourentire vocabulary as well as they'rehaving the special metries WQ WK WVwhich we'll talk about later let's havea look inside the encoder and reviewthis specific two steps so you give asentence to your Transformer and it willfirst tokenize it tokens are kind oflike words but for a word called therewill be two tokens call and Ed so itwill tokenize it and there are variousways to tokenize your sentence this isone of the ways it will also add specialtokens at the end and at the beginningCLS and sep sep is for separators so ifyou have two sentences between twosentences there will be a separator andCLS will be added at the beginning andthis I'm talking about bird then it willalso generate token IDs so for each ofthese words there will be an index intoyour vocabulary for example made is2532 which means in your entirevocabulary which is just like a listmade word is at position2532 if you talk about bird it has total30,00 , 522 tokens and GPT has around50,000 tokens from these token ID sostep number one was generate token andtoken IDs then you uh get the staticembedding for each of these tokens andfrom where do you get it well we justsaw right during the training you aregenerating this static word embeddingmetrix so for each of the words ortokens you havethe static word embedding so in the caseof bird the size of this will be 768 ifit is GPT it will be 12,000 you knowthat long embedding metrics so youproduce that for each of the tokens andthen you will also create somethingcalled a positional embedding now in thelanguage the word order matters okay soif I put made before I it will changethe meaning of that sentence so theorder matters and the way Transformerworks is it will process the entireinput or sequence in parallel it is notlike RNN where it will process thesewords one by one it will process thissequence All In Parallel now it needs tohave knowledge on the order okay so forthat it uses a special technique calledpositional embedding where it will add asmall Vector in each of these embeddingsokay so let's say this is the vector forposition number one this is the vectorfor position number two and so on andwhen you get uh this resulting Vectorthis Vector will embed the knowledge ofposition so this Vector will have aknowledge that this is the first wordthis Vector will have a knowledge thatthis is the second word now how exactlythat is done well there is a math behindit I'm not going to go into the math butI'm showing you the formula from theoriginal Transformer paper so using thisformula you are essentially uh derivingall these positional embeddings allright so that was step two the firststep was to produce the static embeddingfor each of the tokens and then thesecond step is to add positionalembedding like this is a plus sign sohere at this point what you get is thiskind of positionembedding just like how my nephew needsmy attention words also need attentionof surrounding words in order to producethe contextual embedding in2017 a groundbreaking research was donewhen this paper attention is all youneed was published by bunch of Googleresearchers and that completelytransformed the landscape of AI okay andand this is the architecture that we aretalking about the architecture is takenfrom this attention is all you needpaper so the way it works is when youhave this sentence the word Indian needsattention from Dosa Dola Etc you can saythat all these words are attending tothis word Indian even the word b insteadof B in let's say here if I had Dil thiswill become Italian instead of Indian sothis word bin is attending to this wordIndian Dosa Dola Etc is also attendingto this word Indian similarly uh wordsSweet Indian rice Etc are attending tothis word dish now how much they'reattending to this word well thatattention weight or attention scoremight be different for example sweetmight be attending to this word dish by36 person let's say Indian is attendingin it by 14 person rice is attending itby 18 person these are the adjectiveswhich will enrich the meaning of worddish on the other hand the word I madeEtc are not enriching the meaning ofword this that much because instead of Iif I had Rahul or moan or David themeaning of this word will not changethat much but instead of sweet if I havespicy all of a sudden the embedding orthe meaning of this dish changes becauseas a next word I will immediately haveBiryani instead of K the goal here is tobuild this kind of attention weight orattention score okay for each of thesewords it's a matrix because for Dish allthe other words in that same sentenceare they are enriching the meaning ofthat word okay so for word dish let'ssay sweet is attending it by 36 personIndian is attending it by 11 person andso on and by the way I have just made upthese numbers just for explanationpurpose the word dish also uh attends tothat word itself right because dishitself has some meaning dish means dishright so that will also attend to itselfso for every word see right now for DishI have all this scores for Rice you willhave scores for Indian for every wordyou will try to compute these attentionscores and then you will use thisconcept of query key and value to uhcome up with the contextual embeddingnow let me explain you query key andvalue by going over analogy let's sayyou're going to a library looking for abook on quantum physics especiallyQuantum computation you might have thisquery that hey I'm looking for thisquantum physics book and this particularperson who is a librarian will use thebook index so he'll go to his computertry to search for that book or maybe hewill go to this rack and locate aspecific rack which has a label quantummechanics okay so for him the key or theindex to locate that book is the labelon the rack you know in library you seelike history drama science those kind oflabels or you have book description okayso based on book description the racklabel you will figure out theappropriate book so The Book Rack bookdescription Etc is called key and thenthe actual book content is your value solet's say you pull this book okay andwhatever content the actual content ofthat book is value let me give youanother example let's say there is acollege professor who wants to write anessay on Quantum Computing and he needshelp help of bunch of students so whenhe talks to these students moan saysthat I know linear algebra Mera saysthat I know quantum mechanics Bob willsay hey I know philosophy same way Kathyknows computer science so here whatevermoan mea Bob Kathy are claiming abouttheir knowledge is called key and whathappens after that is each of thesestudents will start writing an essay soteacher will say okay just go and writeum some bunch of paragraphs so mea moanKathy Bob wrote all these paragraphswhich are called value and then teacherknows that mea knows most about quantummechanics okay so he will take 60% ofmea's content or mea's value he willtake 29% of kath's value becausecomputer science and quantum Computingso that it's kind of related so he willuse 60% of mea's content 29% of Kathy'scontent to formulate that final essay onthe other hand Bob's content he will useonly one person because the query andkey are not matching that much see Bobhas a knowledge on philosophy but ourquery requires Quantum Computing soquery and key we can say they're notmatching in terms of math you can thinkabout Dot produ so let's say dot productbetween query and key Vector is lesslet's say only one person okay but inthe other case mea query and key dotproduct is higher let's say 60% so youwill take 60% of mea's value which isthe essay written by mea on QuantumComputing now same way for our sentencethe query for Dish is I want to knowabout my modifiers okay I'm just givingyou analogy by the way way the realworking is little different but let'ssay you are generating contextualembedding for the word dish and thequery may look something like I want toknow about my modifiers right like myadjectives all these adjectives whichmodifies my meaning and the key will beuh the description that each of thesewords are giving about themselves forexample I will say I'm the subject ofthe sentence made will say I indicate anaction or a verb similarly sweet willsay say I am an adjective describingtaste and so on so these are called keysand based on the dot product betweenquery and key yeah you're trying to findout you know which things are matchingso if if dish wants to know aboutmodifiers I think these are theadjectives which modifies the meaning ofword dish so the score attention scorefor these will be higher whereas thetension score for these will be lowernow once you get all these attentionscores you need value so each of thesewords will now say the value value meansuh the component that it is contributingto that query so I will say Indian willsay the style or origin is Indian sweetwill say The Taste is sweet similarlyall these words will have specific valueand then uh let's consider the values ofonly these four words I mean as such itwill use values of all the words but forsimpl let's say only these four wordsthese values by the way will be somekind of vector we'll look into howexactly those vectors are derived butlet's say these values are all thesevectors and query also has like dishalso has its own Vector right like thisis the static embedding so this is itsown vector and now what you do is instatic embedding you add all thesevectors and all these vectors you canthink about as ress indianness okay sosee this is how you add all of them okayyou add all of them actually the vectorof all the other words and you get thefinal context of where embedding interms of the embedding space it is likegoing from dish to ress indan ress andso on so these vectors right ressindianness sweetness are these vectorsokay this is just a mathematicalrepresentation now let's look at howthose vectors are built so here you havea query for Dish okay so let me justrepresent it as a horizontal right thiswas a vertical format this is horizontalformat the same thing for each of thesewords or tokens you will first get theirembedding from our stating embeddingMatrix okay so these are staticembeddings for each of these words inthe case of bird the dimension is 768for GPT is 12,000 something let's sayfor word dish this is my embedding let'scall it E7 that E7 you will multiplywith a special Matrix called WQ whichwill have adimension uh of 64 by 768 so 768 is thecolumns in order to perform matrixmultiplication The Columns in the firstMatrix should be equal to rows in thesecond Matrix so this is 6 768 this is768 the rows in The Matrix the firstMatrix is 64 for bir for GPT isdifferent and when you douh this kind of matrix multiplicationyou will get uh this quy Vector okay soyou will multiply this row with thiscolumn okay so you multiply 50 with this0.9minus5 with1.07 65 with this and then you add themall up you put them here then you takethe second row multiply 23 with thisminus 71 with this 1.58 with this andyou put that here and so on okay so thisis how you build a query Vector now WQhere knows how to encode query of atoken for attention computation when wetrain the model we already got the WQand WQ after the training is done it itdoesn't change okay after you do thattraining sometime it is referred aspre-training on huge amount of data youbuild this WQ Matrix which doesn'tchange okay so for a train model uh thisWQ will not change you multiply thatwith specific embedding E7 let's saythis is a positional embedding you getQ7 which is the query Vector for theword dish and you repeat the sameprocess for all the words okay so howyou have Q7 for dish for Rice you willhave q6 Indian you have uh Q5 and so onto summarize WQ here knows how to encodequery of a token for attentioncomputation and remember in one of theprevious slides I said that when themodel is strained it will have staticembedding metrix but it will also havethis WQ WK WV and that is what I wasreferring to okay so we just talkedabout WQ here the question now is duringthe training how exactly we get WQ WK WVwell we take this Transformerarchitecture and we train it on hugeamount of data so we take all theWikipedia text and we generate this kindof X and Y pairs okay so you don't haveto manually label it this is called uhself-supervised data set uh you don'tneed a person to label it because youcan just split a sentence you can have asentence and the next word is your yokay so this is your X this is your yyou feed X as an input and when themodel is not train TR it will notpredict right things it will make errorso let's say for this it produce Mexicanwhich is your why hat okay it's apredicted value your actual value isIndian so that is why you calculateerror and then you back propagate thaterror through back propagation and chainrule partial derivative and so on folksyou need to have understanding of howback propagation Works what is a chainrule you need to know all those deeplearning fundamentals okay I havecovered that in other modules if you'repart of my courses or boot camp youwould have seen those if you're watchingit from YouTube Again YouTube has uhthese kind of tutorials my channel hasthese tutorials so you need to know howthe back propagation Works essentiallyyou are feeding this data set you'reComputing the error and you're backpropagating it throughout thisarchitecture and during that backpropagation when let's say you trainthis on millions and millions ofsentences that is the time when uh thisWQ WK WV will be finalized inside thismodel architecture now going back forDish query we computed this particularquery Vector next step is to compute thekey vectors okay so I gave this kind ofanalogy description to uh get you anintuitive understanding but in realitythese will be the vectors so let's seehow those vectors are formed so here I'mtaking the first token I and the keyslook something like this okay so hereyou will take the positional embeddingthe static embedding for the word I andyou will multiply that with anothermagical Matrix WK once again WK afteryour pre-training after that model istrained it is fixed so you take thatMatrix and you uh figure out your K1okay here WK knows how to encode key ofa token for the attention computationthen you go to the next word compute K2next word compute K3 you do that for allthe words so now for all these words wehave these key vectors okay so you haveQ7 uh query Vector you have key vectorsand you take the dot product betweenthese two okay so q1 K1 Dot Q7 okay soif you take these dot product betweenthese two vectors you'll get some numberright like3.33 57 101 whatever that number is itit's a single number you will get thatfor all the tokens okay and then you letit pass through a soft Max function fromDeep planning fundamentals you shouldknow about softmax softmax will convertbunch of values into probability distribution so that when you add all thesevalues it will be one so soft Max isconverting all these discrete valuesinto probability distribution so thatyou can express them as percentages andthe sum of all these percentage will beone mathematically you can representthis operation as soft Max between q andKT now KT is K transpose okay so here Qwas a vector but if you talk about let'ssay this K right so K is k1 K2 K3 soit's not just one vector actually it'slike bunch of vectors so this can bethought of as a matrix and to multiplythat you need to do a transpose see ifyou are multiplying Q7 with K1 like asingle Vector you don't need to dotranspose but when you have Matrix youneed to do transpose okay so we'll usethis formula later on in the finalattention formula but for now justremember that there is this kind offormula as a Next Step once you havecomp Ed these attention scores orattention weights you need to find thevalue Vector right so this was adescriptive uh understanding of valueVector but the way value vectors arederived is similar for each of thetokens you get positional embeddingstatic embedding then you multiply thatwith another Vector called WV you get V1and here WV knows how to encode value ofa token for attention computation okayso you do that for all the words so V1V2 V3 V4 V7 and so on okay so for allthese words you will uh get their valuesand you multiply that with the weight soyou will have more component from thisV4 Vector because it's like 36 personbut the component that you will use fromV1 will be very less 7 person okay sosee the sweetness you're taking36% uh here I don't have things in orderbut you essentially add all the vectorsokay so you just add all of thiseverything okay so here I'm not showingeverything but you kind of get an ideaso from static embedding you go all theway to context aware embedding here'sthe mathematical formula for attentionqk V where DK is a dimension of a keyvectorin case of GPT this is 128 so what theydo is they take um the entire 12228 Dimension right for GPT thedimension of the contextual embeddingsis 12 228 and you divide it by thenumber of attention heads I think forGPT is 96 and that's how you get 128 Iwill explain this 96 a little later butthere is a way to derive this number 128so you do division by square root ofthat just for numerical stability youdon't want this dot products to becomevery high okay so to bring down thatnumber we do kind of scaling here andyou do soft Max and you multiply thatwith this value V so far what we talkedabout is a single attention blockactually there are multiple attentionblocks so that's what we'll cover nextlet's understand what is multi headattention so far we have seen thispicture where you take positionalembedding for each of the words in yourinput sequence you let it go throughattention head which is basically takingthis WQ WKWV and coming up with context our awareembedding so that whole portion iscalled one attention head in reality youhave multiple attention heads okay soyou have multiple attention heads eachof these heads are producing their owncontext aware embedding which you willadd them up all together to get thefinal context aware embedding now whatis the purpose of this multipleattention heads one attention head willbe working on adjectives okay so for theword dish sweet Indian rice Etc areadjectives the second attention headmight be working on on a verb okay sohow this verb made uh affects thecontextual embedding of the word dishthe third attention block might belooking at pronoun so you can think ofthis as looking at different aspects ofa language or different aspects of thatcontext okay for the other sentence thefirst attention head might be looking ata cultural context such as Dosa DolaMillet bread are all Indian Delicacieswhereas the second attention head mightbe looking at the pronoun where insteadof the and B if I exchange the order ofthese two uh here you will have Italiansimilarly instead of you if I say Iagain here you will have a differentword so there is a pronoun context thethird attention uh head might be lookingat action and timing you know you'redriving 20 minutes Drive Etc so thepurpose of multi- attention heads is toallow the modelto focus on different aspects ordifferent types of relationships betweentokens in a language when you havemultiple tokens there is a differenttype of relationship between thesetokens such as semantic positionalsyntactic uhsimultaneously uh enriching thecontextual understanding of each uhtoken so I want you to read thissentence again uh I hope hope you get anidea it is basically looking atdifferent aspects or differentrelationship between the tokens toenrich thecontextual understanding of each tokenso here in this particular architecturediagram see first we produced this uhstatic embedding then we added thispositional encoding right so you gotpositional encoding here uh you ignorethis normalization part for nownormalization is simple actually it'slike uh normal izing it to Value whichis zero mean and one standard deviationand then looking at v k q kind of metrixto uh use multi-headed attention toderive ec1 ec2 theseindividual uh contextual embedding andyou add all of them up to produce yourfinal context of our embedding whichwill come here and by the way this is aresidual connection uh if you know aboutdeep learning you will have uh this umresidual connection that helps you uhwith a smooth gradient flow after thisblock the next block is feed forwardNetwork so you'll ask me okay I alreadyhave context over embedding now why do Ineed this feed forward Network well thething is you don't have your finalcontext aware embedding yet so here atthis pointthe embeddings are enriched but they arenot still fully furnished yet you haveto let it go through this feed forwardNetwork so what happens is you passedyour positional embedding through bunchof attention heads and you got thisenriched contextual embedding that willgo through a fully connected neuralnetwork layer okay so here the inputneurons will be same number of uhelements as this embedding so in case ofbir let's say this will be768 for GPT it will be 12228 and then in the hidden layer you canhave uh n n number of neurons and in theoutput layer again you'll have same asthis one because this input and thisoutput will have a same size so if thisis 768 this will also be 768 okay so youlet it go through this feed forwardNetwork and the resulting embedding thatyou get is even more enrich it's like amore furnac product now this neuralnetwork weights you know this will havea lot of weights and parameters thoseweights and parameters are set onceagain during that training process sowhen you're going through this XY pairsright your training pairs you might havehundreds and thousands of thesesentences when you're training thatNetwork during that training look atthis feed forward Network you knowduring back propagationthose weights are getting adjusted andit will help you refine your sentencefurther now once you get enrichedembedding you will add that into youroriginal embedding and you get the finalnow it is final now it's a finalcontextually Rich embedding so thepurpose of feed forward network is itwill enrich each token embedding byapplying nonlinear transformationbecause in the attention head you areapplying linear transformation here youget an opportunity to apply nonlineartransformation independently enablingthe model to learn complex patterns andhigher order features Beyond just thecontextual relationship see multi-headattention is just capturing thosecontextual relationships how these wordsare related to each other but languageis nonlinear it's not just therelationship right there are like somenuances nonlinearity complexity all ofthat can be captured by this fullyconnected layer or feed forward Networkso to better visualize each of the wordsin your sentence let's say you have Imade dish every word will go throughpositional embedding and everypositional embedding goes throughmultiple attention head so the embeddingfor I will go through all the heads okayso in GPT if you have 96 heads it willgo through all those 96 heads similarlymade will also go through 96 heads andthis this is happening in parallel it'snot like you process I first and made noall of these things are happening inparallel and each of these uh vectorswill also go through the feed forwardNetwork parall at the same time right sothe same network is available for eachof these words and you get all thesecontextually enriched embeddings okay sothat comes here so after feed forwardNetwork here at this point you get allthese m Bings okay and then you havethis uh plus sign and normalization sonormalization layer by the way this Normis uh just it's ensuring that you havestable learning improving the gradientflow if you have deep learningfundamentals you will understand what Imean uh in machine learning generallywhen you have all these wide range ofvalues if you normalize them let's sayyou normalize them to zero and one youget better control over your trainingnow you also notice this anx layers soanx layers is basically for B let's sayif you have a b base model you have 12such layers okay so this is aTransformer block so you kind of repeatso you have one block then after thatyou have another block so in case of BTbase model you have 12 layers B largeyou have 24 layers in case of GPT againthere will be different number of layersso that's what this NX layers means allright f finally we are done withunderstanding encoder I just want tosummarize we had an input sequence wegenerated a static embedding here thenhere we generated a positional embeddingthen we have one Transformer block or NXlayer where we first normalize we usevkv uh to compute attention score orattention weight we have multiple suchheads and then you go throughnormalization you have feed forwardNetwork you kind of ADD remember likeyou have original embedding and then youadd that output and you get the finalcontextual embedding you normalize itand here at this point you are getting afinalcontextual uh very enriched embedding wehave covered most of the Transformerarchitecture decoder is not going totake uh much time so let's spend fewminutes understanding decoder so theoutput of encoder is a contextualembedding or context Rich embeddingwhich you give it as an input to decoderand decoder will produce the next wordif you're working on next wordprediction if you're working on languagetranslation it will uh start with thisspecial token called start and then itwill produce May then another work herebanay and so on okay so that's a goal ofa decoder now here you will notice onething which is calledmulti-headed cross attention okay solet's understand what exactly is crossattention let's say you have thissentence I made kir which you want totranslate into Hindi here you will havekey vectors and value vectors as we havediscussed before but the query Vectorwill be little different so query Vectorwill be start and it will be like I'mstarting to generate translation whatpart of the input should I focus on andthen when you have next word which isman which is the first word in yourtranslation it will be like I generatedthe subject what is the subject okaythen you will have here I generated thesubject and object help me complate thesentence with a verb form so the querypart is little different see in theprevious example you had only onesentence so I made key so you'llgenerate a query from made let's say andyou will have key and value from samesentence in case of language translationit's little different uh so here we needto use cross cross attention why crossattention because query you are using itfrom the translated sentence in Hindiwhereas key and values are being usedfrom the original sentence in English sothat is why it's called cross attentionin the diagram you can see here the Vand K values are coming from yourencoder encoder has processed this imhere okay so V and K are coming fromencod see this is the arrow whereas Q iscoming from the decoder itself right soyou know that Hindi sequence will beproduced here so man K banai Etc so thatquery part is coming from here that iswhy it is called cross attention in thecase of B we all know that there is onlyencoder part decoder part is not therein case of GPT the Transformerarchitecture is little different okay sothat's the encoder part remaining partwe have already understood now let me uhshow one nice tool which can visuallyshow you this architecture someone hasbuilt this nice visualization tool youcan go to Pol club. github.ioTransformer explainer and here you canlook into different examples right sofor example let's look at this sentenceas the space ship was approaching the itwill try to autocomplete that word andsay station right now each of thesewords has the space ship first you hadthis Dropout layer so we talking aboutTransformer here so it will have aDropout layer the architecture is littlecustomized compared to the baseTransformer architecture then you havethis residual connection okay soresidual connection will take you fromhere to here if you talk aboutembeddings you have token embeddings foreach of these right see 768 is the sizethen you have positional embedding okayyou add all this position and you getfinal Vector this is apositional embedding of a sentence andyou have residual connection then youhave q KV computation so if you look atthis particular block here qkv it willkind of visually show you how youcompute q k v and get all those threevectors right like query Vector keyvector and value vector and then uh youhave this output which you feed it toMLP is your feed forward neural networkthat we talked about okay so feedforward neural network then you againhave a residual block and then you havelayer normalization and this is oneTransformer block you have multiple ofthem like 11 okay so that Annex layerthat I was referring to is one block soyou have repeated blocks and in the endyou get this kind of soft Maxprobability see the probability here isa station okay so just play with thisparticular Tool uh to get a betterunderstanding of this thing and I wantto give credits uh to this amazingChannel called 3 blue one brown so ifyou go to YouTube and type inTransformer explain 3 blue one brown youwill find all these videos so I want youto watch from video number 5678 onwardshe will have more videos as well uhespecially these three video dl5 dl6 dl7these three videos you must watch itwill enhance your understanding furtherI myself learned a lot from this channelso due credits to three blue one brownall right that's it folks so that's thatwas about Transformer I know it was along discussion there were many topicsthat we covered but hopefully yourunderstanding is clear if you have anyquestion please feel free to ask[Music]a[Music]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  ytt_api = YouTubeTranscriptApi()\n",
        "  fetched = ytt_api.fetch(video_id,\n",
        "                          languages = ['en'])\n",
        "  raw_transcript = fetched.to_raw_data()\n",
        "  transcript = \"\".join(entry['text'] for entry in raw_transcript)\n",
        "  print(transcript)\n",
        "except TranscriptsDisabled:\n",
        "  print(\"No captions available for this video\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "eNpx-CwRGHlQ",
        "outputId": "beee21c5-b0fa-4b5d-e881-656043440eb0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Chad GPT is powered by a model calledGPT which is based on a deep learningarchitecture called TransformersTransformers is the reason behind modernday AI boom as an AI Enthusiast when youstart learning Transformers you willcome across this complex diagram whichwill start giving you a headacheimmediately my goal for today's video isto explain you Transformers in a mostsimplified and intuitive manner we needto cover many different topics so thisis going to be a long video attentionand patience is all I need from youtoday when you type in a sentence inGmail it tries to predict next word ornext set of words this is possiblebecause of a machine learning modelcalled language model Google for examplehas this popular language model calledbird which is powering hundreds of AIapplications throughout the world GPTwhich is a model behind chat GPT is alarge language model the reason it iscalled large language model is becauseit has billions of parameters it is muchmore capable and Powerful compared tobird and it is trained on humongousamount of data fundamentally though itis also doing the same thing which iswhen you type in a question in chat GPTit will predict the next word in thatsentence and then it will take theoriginal question and the next predictedword as an input and then predict thenext word and then the next word and soon in the end it produces is a completeanswer which almost sounds like a magicto summarize the goal of a languagemodel is to predict a next word in asentence now that we have understoodthis fundamental let's look into some ofthe topics which needs to be clarifiedbefore we dig into the actualarchitecture the first concept we needto understand is word embedding machinelearning models do not understand textthey understand numbers so we need torepresent text as numbers let's say youhave this word King you want tonumerically represent it how would youdo that well you can assign a fixednumber you can have a vocabulary and youcan assign just fixed static number butthat will not capture the meaning of itwhen you're building a language modelyou have to represent words in such away that they capture the meaning ofthat word one way to capture the meaningof this word King and represent itnumerically is to ask bunch of questionsfor example does this person hasAuthority yes one do they have a tail nohorse has a tail King doesn't have atail are they Rich yes gender minus oneis male one is female and so on what wejust did is we created this Vector listof numbers which is a vector torepresent this word King similarly wecan represent the word queen as well notonly that we can can represent bunch ofwords such as battle horse King and soon by asking set of questions okay sofor battle they don't have authority arethey an event yes do battle has tail nobattle is an event it it doesn't havetail and so on similarly horse do theyhave authority well we'll just say 01 ifit is King's horse maybe they have someAuthority or maybe they have authorityover their horse kids and so onsimilarly we can represent all thesewords in a numeric format and then wecan take the vector of this word Kingand maybe we can do some mathematicswith it we can say King minus man sohere I'm taking the vector of man rightwhich is uh this particular Vector pluswoman which is this particular vectorand when you do the math which is like 1- 2 + 2 will beZ and so on you get a vector which lookssimilar to Queen now this sounds like amagic we can do math with Words which isKing minus man plus woman is equal toQueen here King was represented in fiveDimensions when you look at the reallife for example Google's word to wackmodel it has 300 dimensions and what areall these questions by the way well weactually don't know this has beentrained through a neural network and wehave processed huge amount of text suchas all the Wikipedia articles all thebooks and text on internet to understandthe relationship between these words andthrough that neural network trainingback propagation we came up with thisVector the example that I gave for Kingwhere we asked these questions Authorityand so on that was just a madeup examplefor building intuition for wordembeddings in real life we do not knowwhat all these number means all we cansay is these numbers are the featuresfor this word and they capture themeaning of this word King let's say kingis a three-dimensional embedding if youhave to represent that in this 3Dembedding space you can represent itlike this where x axis has three likethis three number y AIS has this eightnumber z-axis has this two number and soon I use three dimensions because ashumans we can view only three dimensionswe can't possibly view this 300Dimension okay but mathematically those300 dimensions are possible models likeGPT uses an embedding Vector which iseven 12,000 Dimensions okay so it's avery rich High dimensional space that weare working with in threedimensionalspace you can have vectors for King kingand queen that looks something like thisand if you look at this Vector which isjoining king and queen you can think ofthat as a gender Direction the benefitof this gender Direction Vector is thatwhen you have another embedding forUncle you can add that gender Directionand get the embedding for word Auntsimilarly if you have father you can getmother if you have man you can uh getwoman and so on and that allows you todo this amazing math such as king minusQueen plus uncle is equal to Auntanother example is you can have countryto Capital City Direction Vector whichyou can use to add it in this embedingof Russia to get the embedding of Moscowyou can do Russia minus Moscow plusDelhi equal to India now the embeddingthat we're talking about are staticembeddings wtu and glow are two popularmodelswhich helps you get the static embeddingstatic means fixed embedding for allthese words you may ask how theseembeddings are generated well I alreadyanswered the question which is you traina neural network model on humongousamount of text Wikipedia books and so onto understand the relationship betweenthe words I'm not going to go into themathematical details of word to you canrefer to some other material on internetI have YouTube videos for that I'm notgoing to go into the math of that butjust think that uh the neural networktries to understand the relationshipbetween these words and creates thesestatic embeddings now the problem withstatic embedding is that you can have astatic embedding for this word track butbased on a sentence that track can memean different things right like hereI'm saying the train will run on thetrack and my package is late help metrack it so the meaning of track islittle different and when you havestatic embedding you get into thisproblem where you're not able torepresent this word properly based onthe context of this sentence you willsee same issue here for Dish you canhave a fixed embedding but in thissentence I'm talking about rice dish ifI had a cheese dish then the embeddingof dish should be a little differentbecause the meaning of that dish word islittle different when I say rice dishversus cheese dish when you are workingon predicting the next word for thissentence you can have words such asrisotto itly Mexican rice but when I sayI made an Indian rice dish call all of asudden the probabilities of my nextwords will change I will have words suchas idly Biryani ke if I add one moreadjective and say I made a sweet Indianrice dish in that case again it willchange I will not have Biryani as a nextword prediction I will probably have Kor Pongal to summarize to build anapplication like CH GPT just the staticword embeddings are not enough what youneed is contextual embedding let'sunderstand contextual embedding a bitmore in detail when you represent thisword dish in your embedding space it isaesthetic embedding when you say risethis maybe there is another Vector inthe same space which can accuratelydescribe rice dis or which can correctlycapture the meaning of word rice dishwhich can be Roto Biryani and so on thedirection from dish to rice dish we cancall it ress and when you add that ressVector to Dish what you get is theembedding for a rice dish there isanother vector or embedding for Indianrice dis and to go from Rice dis toIndian rice dis you need to probably addthis vector or a direction calledindianness and same thing for sweetIndian rice dish in order to generatecontextual embedding what we need to dois take the original static embeddingfor the word dish and have all theseother words influence that staticembedding or change that staticembedding so that it can capture themeaning of all these adjectives once youhave done that and once you have a acontextual embedding for dash it won'tbe hard to predict the next word whichis K look at this another sentence whereI'm saying D loves Dosa Dola and Milletand so on B loves pasta and so on theyboth went out for a dinner and here BNsaid bro we'll go to a restaurant thatyou like and after some time they werein Indian restaurant now the way youpredicted this word Indian was based onthis contsuch as D LS all these items which arepart of the Indian Cuisine also bin saidto D bro will go to a restaurant youlike if bin said here instead of you ifhe had said I this word will becomeItalian instead of Indian right alsoinstead of B if there was double herethen also this thing will become Italianso you can understand that this uhprediction Indian uh is influenced bynot just the few words which are priorto that word but it can be influenced bysome words which are far out in thatparagraph okay to summarize theobjective for this intelligent teachercat is to generate a contextualembedding and if you think about thisembedding space mathematically speakingwhat you're doing is taking the staticembedding for the word Dash and thenadding the embeddings for all thesevectors ress indianness all theseadjectives and getting your finalcontextual embedding let's now dig intothe Transformer architecture thearchitecture has two components encoderand decoder the purpose of encoder is totake the input sentence and generate thecontextual embedding for each of thewords or each of the tokens in thatsentence once the contextual embeddingis generated we feed that to a decoderhere and we try to predict the next wordso if you're working on the next wordprediction you will predict the nextword here for example it will be herewhen you talk about natural languageprocessing there are multiple tasks soone task is to predict the next word theother task would be to translate thesentence here I'm translating thesentence from English English to Hindiin that case you will still produce thecontextual embedding from encoder youfeed that to decoder and decoder willstart predicting the next word so hereit will start with this fixed starttoken and then it will uh produce theprobability of the next word so here theprobability of this word man is highestand here you can have the entirevocabulary for example in the case ofbird you have some 30,000 words soyou'll have all the words in yourlanguage and you will say okay what isthe highest probability of my next wordthen you put that word man into thisinput okay so there are two inputsactually one is the contextual embeddingwhich is coming uh from here from theencoder and the other one is whateveroutput you have produced so far from theprevious step you feed that okay as aninput here and then it will produce thenext word which is K once again youprovide key here and it produces banaiSo eventually it produces the entiretranslated sentence all right so that'sthe objective of your encoder anddecoder part whatever I talked about sofar I was referring to an inferencestage of uh neural networks whenever youhave these uh deep learning model youhave two stages one stage is the modelis not trained it's like a baby baby isnot trained yet and you train them rightyou send them to school you train themuh in your home at some point theybecome adult and they can figure thingsout on their own similarly a machinelearning model goes through a trainingphase and when it is ready it startsworking on the real world problem andthat is called inference so whatever Italked about for for predicting the nextword or translating sentence I wasreferring to inference stage throughoutthe discussion we'll be referring to twospecific models called bird and GPT ifyou look at this architecture that's ageneric architecture for a Transformermodel Transformer model is a generalconcept whereas bird and GPT arespecific model or specificimplementations based on Transformerarchitecture if you look at birdarchitecture it has only the encoderpart okay so only this part decoder partis missing so but will take the inputsentence it will produce the contextualembedding and that's it whereas GPT hasonly decoder so it still takes the inputit will produce the contextual embeddingand so on and then here it will predictthe next word I mean it sounds like ithas encoder decoder both butfundamentally the architecture lookslittle different for GPT but it is stillbased on the Transformer architecturethe way they're trained is you take allthe text from Wikipedia crawl text frominternet book Corpus and you train thesemodels when you have this article forexample and you are having this sentencedeveloping an advanced crude see if Igive you this sentence most likely youwill say spacecraft or vehicle as a nextword you'll not say banana right like soprobability of having banana as a nextword in this sentence is very lesswhereas these two words have a highprobability so we as humans have read somuch text so now we have learned thisart of predicting next word and samething goes on for B and GPT where theyunderstand the relationship between thewords the context in which they appearso let's say if B during the traininghas encountered so much text and everytime after this word crude if it hasseen this word spacecraft of or vehicleuh it would not have seen words likecrude chair or crude banana right thatkind of words usually when it is goingthrough training it will not see it soit will learn to predict the highprobability worse right so for word likebanana probability is going to be lowersame thing for this article when you gothrough this kind of sentences right SIengaging both alliances and hostilitiesand there will be many more artic onbattles and Warriors and everywhereafter alliances there will be eitherhostilities or negotiations there willnot be a word like chair so probabilityof that will be very very low now whenyou go through the training you aregoing through all these words right soall these words will form somethingcalled a vocabulary so for a Model Avocabulary will look something like thisnow there is a difference between a wordand a token so for example here playingis a word and one of the way to tokenizeis to have two token so one token isplay second token is ing okay so tokenwise there are two tokens but word isjust once but just for uh understandingpurpose just for easy explanation youcan think of word as tokens technicallythey're different but you can think ofwords as token only okay so let's sayyou have a vocabulary of all thesetokens let's say30,000 words what happens here is foreach of these words during the trainingit will create those static embeddingsso for word made or let's say for wordand seven is the index and let's saythis is the static embedding vector andthe dimension or the size see there is adot dot dot so what is the size of thiswell for bird it is 768 for GPT it's12,228 right so based on model thedimension of your embedding Vector canvary when you go through this trainingfor every token in your vocabulary youwill have this static embedding and thiswhole table is called Static embeddingMatrix during the training it will alsolearn few other things such as WQ WK WVand you are like what the hell this iswell we will talk about this later butfor now just remember when you trainthese models they are having this staticword embedding metrix which is thestatic embedding for every token in yourentire vocabulary as well as they'rehaving the special metries WQ WK WVwhich we'll talk about later let's havea look inside the encoder and reviewthis specific two steps so you give asentence to your Transformer and it willfirst tokenize it tokens are kind oflike words but for a word called therewill be two tokens call and Ed so itwill tokenize it and there are variousways to tokenize your sentence this isone of the ways it will also add specialtokens at the end and at the beginningCLS and sep sep is for separators so ifyou have two sentences between twosentences there will be a separator andCLS will be added at the beginning andthis I'm talking about bird then it willalso generate token IDs so for each ofthese words there will be an index intoyour vocabulary for example made is2532 which means in your entirevocabulary which is just like a listmade word is at position2532 if you talk about bird it has total30,00 , 522 tokens and GPT has around50,000 tokens from these token ID sostep number one was generate token andtoken IDs then you uh get the staticembedding for each of these tokens andfrom where do you get it well we justsaw right during the training you aregenerating this static word embeddingmetrix so for each of the words ortokens you havethe static word embedding so in the caseof bird the size of this will be 768 ifit is GPT it will be 12,000 you knowthat long embedding metrics so youproduce that for each of the tokens andthen you will also create somethingcalled a positional embedding now in thelanguage the word order matters okay soif I put made before I it will changethe meaning of that sentence so theorder matters and the way Transformerworks is it will process the entireinput or sequence in parallel it is notlike RNN where it will process thesewords one by one it will process thissequence All In Parallel now it needs tohave knowledge on the order okay so forthat it uses a special technique calledpositional embedding where it will add asmall Vector in each of these embeddingsokay so let's say this is the vector forposition number one this is the vectorfor position number two and so on andwhen you get uh this resulting Vectorthis Vector will embed the knowledge ofposition so this Vector will have aknowledge that this is the first wordthis Vector will have a knowledge thatthis is the second word now how exactlythat is done well there is a math behindit I'm not going to go into the math butI'm showing you the formula from theoriginal Transformer paper so using thisformula you are essentially uh derivingall these positional embeddings allright so that was step two the firststep was to produce the static embeddingfor each of the tokens and then thesecond step is to add positionalembedding like this is a plus sign sohere at this point what you get is thiskind of positionembedding just like how my nephew needsmy attention words also need attentionof surrounding words in order to producethe contextual embedding in2017 a groundbreaking research was donewhen this paper attention is all youneed was published by bunch of Googleresearchers and that completelytransformed the landscape of AI okay andand this is the architecture that we aretalking about the architecture is takenfrom this attention is all you needpaper so the way it works is when youhave this sentence the word Indian needsattention from Dosa Dola Etc you can saythat all these words are attending tothis word Indian even the word b insteadof B in let's say here if I had Dil thiswill become Italian instead of Indian sothis word bin is attending to this wordIndian Dosa Dola Etc is also attendingto this word Indian similarly uh wordsSweet Indian rice Etc are attending tothis word dish now how much they'reattending to this word well thatattention weight or attention scoremight be different for example sweetmight be attending to this word dish by36 person let's say Indian is attendingin it by 14 person rice is attending itby 18 person these are the adjectiveswhich will enrich the meaning of worddish on the other hand the word I madeEtc are not enriching the meaning ofword this that much because instead of Iif I had Rahul or moan or David themeaning of this word will not changethat much but instead of sweet if I havespicy all of a sudden the embedding orthe meaning of this dish changes becauseas a next word I will immediately haveBiryani instead of K the goal here is tobuild this kind of attention weight orattention score okay for each of thesewords it's a matrix because for Dish allthe other words in that same sentenceare they are enriching the meaning ofthat word okay so for word dish let'ssay sweet is attending it by 36 personIndian is attending it by 11 person andso on and by the way I have just made upthese numbers just for explanationpurpose the word dish also uh attends tothat word itself right because dishitself has some meaning dish means dishright so that will also attend to itselfso for every word see right now for DishI have all this scores for Rice you willhave scores for Indian for every wordyou will try to compute these attentionscores and then you will use thisconcept of query key and value to uhcome up with the contextual embeddingnow let me explain you query key andvalue by going over analogy let's sayyou're going to a library looking for abook on quantum physics especiallyQuantum computation you might have thisquery that hey I'm looking for thisquantum physics book and this particularperson who is a librarian will use thebook index so he'll go to his computertry to search for that book or maybe hewill go to this rack and locate aspecific rack which has a label quantummechanics okay so for him the key or theindex to locate that book is the labelon the rack you know in library you seelike history drama science those kind oflabels or you have book description okayso based on book description the racklabel you will figure out theappropriate book so The Book Rack bookdescription Etc is called key and thenthe actual book content is your value solet's say you pull this book okay andwhatever content the actual content ofthat book is value let me give youanother example let's say there is acollege professor who wants to write anessay on Quantum Computing and he needshelp help of bunch of students so whenhe talks to these students moan saysthat I know linear algebra Mera saysthat I know quantum mechanics Bob willsay hey I know philosophy same way Kathyknows computer science so here whatevermoan mea Bob Kathy are claiming abouttheir knowledge is called key and whathappens after that is each of thesestudents will start writing an essay soteacher will say okay just go and writeum some bunch of paragraphs so mea moanKathy Bob wrote all these paragraphswhich are called value and then teacherknows that mea knows most about quantummechanics okay so he will take 60% ofmea's content or mea's value he willtake 29% of kath's value becausecomputer science and quantum Computingso that it's kind of related so he willuse 60% of mea's content 29% of Kathy'scontent to formulate that final essay onthe other hand Bob's content he will useonly one person because the query andkey are not matching that much see Bobhas a knowledge on philosophy but ourquery requires Quantum Computing soquery and key we can say they're notmatching in terms of math you can thinkabout Dot produ so let's say dot productbetween query and key Vector is lesslet's say only one person okay but inthe other case mea query and key dotproduct is higher let's say 60% so youwill take 60% of mea's value which isthe essay written by mea on QuantumComputing now same way for our sentencethe query for Dish is I want to knowabout my modifiers okay I'm just givingyou analogy by the way way the realworking is little different but let'ssay you are generating contextualembedding for the word dish and thequery may look something like I want toknow about my modifiers right like myadjectives all these adjectives whichmodifies my meaning and the key will beuh the description that each of thesewords are giving about themselves forexample I will say I'm the subject ofthe sentence made will say I indicate anaction or a verb similarly sweet willsay say I am an adjective describingtaste and so on so these are called keysand based on the dot product betweenquery and key yeah you're trying to findout you know which things are matchingso if if dish wants to know aboutmodifiers I think these are theadjectives which modifies the meaning ofword dish so the score attention scorefor these will be higher whereas thetension score for these will be lowernow once you get all these attentionscores you need value so each of thesewords will now say the value value meansuh the component that it is contributingto that query so I will say Indian willsay the style or origin is Indian sweetwill say The Taste is sweet similarlyall these words will have specific valueand then uh let's consider the values ofonly these four words I mean as such itwill use values of all the words but forsimpl let's say only these four wordsthese values by the way will be somekind of vector we'll look into howexactly those vectors are derived butlet's say these values are all thesevectors and query also has like dishalso has its own Vector right like thisis the static embedding so this is itsown vector and now what you do is instatic embedding you add all thesevectors and all these vectors you canthink about as ress indianness okay sosee this is how you add all of them okayyou add all of them actually the vectorof all the other words and you get thefinal context of where embedding interms of the embedding space it is likegoing from dish to ress indan ress andso on so these vectors right ressindianness sweetness are these vectorsokay this is just a mathematicalrepresentation now let's look at howthose vectors are built so here you havea query for Dish okay so let me justrepresent it as a horizontal right thiswas a vertical format this is horizontalformat the same thing for each of thesewords or tokens you will first get theirembedding from our stating embeddingMatrix okay so these are staticembeddings for each of these words inthe case of bird the dimension is 768for GPT is 12,000 something let's sayfor word dish this is my embedding let'scall it E7 that E7 you will multiplywith a special Matrix called WQ whichwill have adimension uh of 64 by 768 so 768 is thecolumns in order to perform matrixmultiplication The Columns in the firstMatrix should be equal to rows in thesecond Matrix so this is 6 768 this is768 the rows in The Matrix the firstMatrix is 64 for bir for GPT isdifferent and when you douh this kind of matrix multiplicationyou will get uh this quy Vector okay soyou will multiply this row with thiscolumn okay so you multiply 50 with this0.9minus5 with1.07 65 with this and then you add themall up you put them here then you takethe second row multiply 23 with thisminus 71 with this 1.58 with this andyou put that here and so on okay so thisis how you build a query Vector now WQhere knows how to encode query of atoken for attention computation when wetrain the model we already got the WQand WQ after the training is done it itdoesn't change okay after you do thattraining sometime it is referred aspre-training on huge amount of data youbuild this WQ Matrix which doesn'tchange okay so for a train model uh thisWQ will not change you multiply thatwith specific embedding E7 let's saythis is a positional embedding you getQ7 which is the query Vector for theword dish and you repeat the sameprocess for all the words okay so howyou have Q7 for dish for Rice you willhave q6 Indian you have uh Q5 and so onto summarize WQ here knows how to encodequery of a token for attentioncomputation and remember in one of theprevious slides I said that when themodel is strained it will have staticembedding metrix but it will also havethis WQ WK WV and that is what I wasreferring to okay so we just talkedabout WQ here the question now is duringthe training how exactly we get WQ WK WVwell we take this Transformerarchitecture and we train it on hugeamount of data so we take all theWikipedia text and we generate this kindof X and Y pairs okay so you don't haveto manually label it this is called uhself-supervised data set uh you don'tneed a person to label it because youcan just split a sentence you can have asentence and the next word is your yokay so this is your X this is your yyou feed X as an input and when themodel is not train TR it will notpredict right things it will make errorso let's say for this it produce Mexicanwhich is your why hat okay it's apredicted value your actual value isIndian so that is why you calculateerror and then you back propagate thaterror through back propagation and chainrule partial derivative and so on folksyou need to have understanding of howback propagation Works what is a chainrule you need to know all those deeplearning fundamentals okay I havecovered that in other modules if you'repart of my courses or boot camp youwould have seen those if you're watchingit from YouTube Again YouTube has uhthese kind of tutorials my channel hasthese tutorials so you need to know howthe back propagation Works essentiallyyou are feeding this data set you'reComputing the error and you're backpropagating it throughout thisarchitecture and during that backpropagation when let's say you trainthis on millions and millions ofsentences that is the time when uh thisWQ WK WV will be finalized inside thismodel architecture now going back forDish query we computed this particularquery Vector next step is to compute thekey vectors okay so I gave this kind ofanalogy description to uh get you anintuitive understanding but in realitythese will be the vectors so let's seehow those vectors are formed so here I'mtaking the first token I and the keyslook something like this okay so hereyou will take the positional embeddingthe static embedding for the word I andyou will multiply that with anothermagical Matrix WK once again WK afteryour pre-training after that model istrained it is fixed so you take thatMatrix and you uh figure out your K1okay here WK knows how to encode key ofa token for the attention computationthen you go to the next word compute K2next word compute K3 you do that for allthe words so now for all these words wehave these key vectors okay so you haveQ7 uh query Vector you have key vectorsand you take the dot product betweenthese two okay so q1 K1 Dot Q7 okay soif you take these dot product betweenthese two vectors you'll get some numberright like3.33 57 101 whatever that number is itit's a single number you will get thatfor all the tokens okay and then you letit pass through a soft Max function fromDeep planning fundamentals you shouldknow about softmax softmax will convertbunch of values into probability distribution so that when you add all thesevalues it will be one so soft Max isconverting all these discrete valuesinto probability distribution so thatyou can express them as percentages andthe sum of all these percentage will beone mathematically you can representthis operation as soft Max between q andKT now KT is K transpose okay so here Qwas a vector but if you talk about let'ssay this K right so K is k1 K2 K3 soit's not just one vector actually it'slike bunch of vectors so this can bethought of as a matrix and to multiplythat you need to do a transpose see ifyou are multiplying Q7 with K1 like asingle Vector you don't need to dotranspose but when you have Matrix youneed to do transpose okay so we'll usethis formula later on in the finalattention formula but for now justremember that there is this kind offormula as a Next Step once you havecomp Ed these attention scores orattention weights you need to find thevalue Vector right so this was adescriptive uh understanding of valueVector but the way value vectors arederived is similar for each of thetokens you get positional embeddingstatic embedding then you multiply thatwith another Vector called WV you get V1and here WV knows how to encode value ofa token for attention computation okayso you do that for all the words so V1V2 V3 V4 V7 and so on okay so for allthese words you will uh get their valuesand you multiply that with the weight soyou will have more component from thisV4 Vector because it's like 36 personbut the component that you will use fromV1 will be very less 7 person okay sosee the sweetness you're taking36% uh here I don't have things in orderbut you essentially add all the vectorsokay so you just add all of thiseverything okay so here I'm not showingeverything but you kind of get an ideaso from static embedding you go all theway to context aware embedding here'sthe mathematical formula for attentionqk V where DK is a dimension of a keyvectorin case of GPT this is 128 so what theydo is they take um the entire 12228 Dimension right for GPT thedimension of the contextual embeddingsis 12 228 and you divide it by thenumber of attention heads I think forGPT is 96 and that's how you get 128 Iwill explain this 96 a little later butthere is a way to derive this number 128so you do division by square root ofthat just for numerical stability youdon't want this dot products to becomevery high okay so to bring down thatnumber we do kind of scaling here andyou do soft Max and you multiply thatwith this value V so far what we talkedabout is a single attention blockactually there are multiple attentionblocks so that's what we'll cover nextlet's understand what is multi headattention so far we have seen thispicture where you take positionalembedding for each of the words in yourinput sequence you let it go throughattention head which is basically takingthis WQ WKWV and coming up with context our awareembedding so that whole portion iscalled one attention head in reality youhave multiple attention heads okay soyou have multiple attention heads eachof these heads are producing their owncontext aware embedding which you willadd them up all together to get thefinal context aware embedding now whatis the purpose of this multipleattention heads one attention head willbe working on adjectives okay so for theword dish sweet Indian rice Etc areadjectives the second attention headmight be working on on a verb okay sohow this verb made uh affects thecontextual embedding of the word dishthe third attention block might belooking at pronoun so you can think ofthis as looking at different aspects ofa language or different aspects of thatcontext okay for the other sentence thefirst attention head might be looking ata cultural context such as Dosa DolaMillet bread are all Indian Delicacieswhereas the second attention head mightbe looking at the pronoun where insteadof the and B if I exchange the order ofthese two uh here you will have Italiansimilarly instead of you if I say Iagain here you will have a differentword so there is a pronoun context thethird attention uh head might be lookingat action and timing you know you'redriving 20 minutes Drive Etc so thepurpose of multi- attention heads is toallow the modelto focus on different aspects ordifferent types of relationships betweentokens in a language when you havemultiple tokens there is a differenttype of relationship between thesetokens such as semantic positionalsyntactic uhsimultaneously uh enriching thecontextual understanding of each uhtoken so I want you to read thissentence again uh I hope hope you get anidea it is basically looking atdifferent aspects or differentrelationship between the tokens toenrich thecontextual understanding of each tokenso here in this particular architecturediagram see first we produced this uhstatic embedding then we added thispositional encoding right so you gotpositional encoding here uh you ignorethis normalization part for nownormalization is simple actually it'slike uh normal izing it to Value whichis zero mean and one standard deviationand then looking at v k q kind of metrixto uh use multi-headed attention toderive ec1 ec2 theseindividual uh contextual embedding andyou add all of them up to produce yourfinal context of our embedding whichwill come here and by the way this is aresidual connection uh if you know aboutdeep learning you will have uh this umresidual connection that helps you uhwith a smooth gradient flow after thisblock the next block is feed forwardNetwork so you'll ask me okay I alreadyhave context over embedding now why do Ineed this feed forward Network well thething is you don't have your finalcontext aware embedding yet so here atthis pointthe embeddings are enriched but they arenot still fully furnished yet you haveto let it go through this feed forwardNetwork so what happens is you passedyour positional embedding through bunchof attention heads and you got thisenriched contextual embedding that willgo through a fully connected neuralnetwork layer okay so here the inputneurons will be same number of uhelements as this embedding so in case ofbir let's say this will be768 for GPT it will be 12228 and then in the hidden layer you canhave uh n n number of neurons and in theoutput layer again you'll have same asthis one because this input and thisoutput will have a same size so if thisis 768 this will also be 768 okay so youlet it go through this feed forwardNetwork and the resulting embedding thatyou get is even more enrich it's like amore furnac product now this neuralnetwork weights you know this will havea lot of weights and parameters thoseweights and parameters are set onceagain during that training process sowhen you're going through this XY pairsright your training pairs you might havehundreds and thousands of thesesentences when you're training thatNetwork during that training look atthis feed forward Network you knowduring back propagationthose weights are getting adjusted andit will help you refine your sentencefurther now once you get enrichedembedding you will add that into youroriginal embedding and you get the finalnow it is final now it's a finalcontextually Rich embedding so thepurpose of feed forward network is itwill enrich each token embedding byapplying nonlinear transformationbecause in the attention head you areapplying linear transformation here youget an opportunity to apply nonlineartransformation independently enablingthe model to learn complex patterns andhigher order features Beyond just thecontextual relationship see multi-headattention is just capturing thosecontextual relationships how these wordsare related to each other but languageis nonlinear it's not just therelationship right there are like somenuances nonlinearity complexity all ofthat can be captured by this fullyconnected layer or feed forward Networkso to better visualize each of the wordsin your sentence let's say you have Imade dish every word will go throughpositional embedding and everypositional embedding goes throughmultiple attention head so the embeddingfor I will go through all the heads okayso in GPT if you have 96 heads it willgo through all those 96 heads similarlymade will also go through 96 heads andthis this is happening in parallel it'snot like you process I first and made noall of these things are happening inparallel and each of these uh vectorswill also go through the feed forwardNetwork parall at the same time right sothe same network is available for eachof these words and you get all thesecontextually enriched embeddings okay sothat comes here so after feed forwardNetwork here at this point you get allthese m Bings okay and then you havethis uh plus sign and normalization sonormalization layer by the way this Normis uh just it's ensuring that you havestable learning improving the gradientflow if you have deep learningfundamentals you will understand what Imean uh in machine learning generallywhen you have all these wide range ofvalues if you normalize them let's sayyou normalize them to zero and one youget better control over your trainingnow you also notice this anx layers soanx layers is basically for B let's sayif you have a b base model you have 12such layers okay so this is aTransformer block so you kind of repeatso you have one block then after thatyou have another block so in case of BTbase model you have 12 layers B largeyou have 24 layers in case of GPT againthere will be different number of layersso that's what this NX layers means allright f finally we are done withunderstanding encoder I just want tosummarize we had an input sequence wegenerated a static embedding here thenhere we generated a positional embeddingthen we have one Transformer block or NXlayer where we first normalize we usevkv uh to compute attention score orattention weight we have multiple suchheads and then you go throughnormalization you have feed forwardNetwork you kind of ADD remember likeyou have original embedding and then youadd that output and you get the finalcontextual embedding you normalize itand here at this point you are getting afinalcontextual uh very enriched embedding wehave covered most of the Transformerarchitecture decoder is not going totake uh much time so let's spend fewminutes understanding decoder so theoutput of encoder is a contextualembedding or context Rich embeddingwhich you give it as an input to decoderand decoder will produce the next wordif you're working on next wordprediction if you're working on languagetranslation it will uh start with thisspecial token called start and then itwill produce May then another work herebanay and so on okay so that's a goal ofa decoder now here you will notice onething which is calledmulti-headed cross attention okay solet's understand what exactly is crossattention let's say you have thissentence I made kir which you want totranslate into Hindi here you will havekey vectors and value vectors as we havediscussed before but the query Vectorwill be little different so query Vectorwill be start and it will be like I'mstarting to generate translation whatpart of the input should I focus on andthen when you have next word which isman which is the first word in yourtranslation it will be like I generatedthe subject what is the subject okaythen you will have here I generated thesubject and object help me complate thesentence with a verb form so the querypart is little different see in theprevious example you had only onesentence so I made key so you'llgenerate a query from made let's say andyou will have key and value from samesentence in case of language translationit's little different uh so here we needto use cross cross attention why crossattention because query you are using itfrom the translated sentence in Hindiwhereas key and values are being usedfrom the original sentence in English sothat is why it's called cross attentionin the diagram you can see here the Vand K values are coming from yourencoder encoder has processed this imhere okay so V and K are coming fromencod see this is the arrow whereas Q iscoming from the decoder itself right soyou know that Hindi sequence will beproduced here so man K banai Etc so thatquery part is coming from here that iswhy it is called cross attention in thecase of B we all know that there is onlyencoder part decoder part is not therein case of GPT the Transformerarchitecture is little different okay sothat's the encoder part remaining partwe have already understood now let me uhshow one nice tool which can visuallyshow you this architecture someone hasbuilt this nice visualization tool youcan go to Pol club. github.ioTransformer explainer and here you canlook into different examples right sofor example let's look at this sentenceas the space ship was approaching the itwill try to autocomplete that word andsay station right now each of thesewords has the space ship first you hadthis Dropout layer so we talking aboutTransformer here so it will have aDropout layer the architecture is littlecustomized compared to the baseTransformer architecture then you havethis residual connection okay soresidual connection will take you fromhere to here if you talk aboutembeddings you have token embeddings foreach of these right see 768 is the sizethen you have positional embedding okayyou add all this position and you getfinal Vector this is apositional embedding of a sentence andyou have residual connection then youhave q KV computation so if you look atthis particular block here qkv it willkind of visually show you how youcompute q k v and get all those threevectors right like query Vector keyvector and value vector and then uh youhave this output which you feed it toMLP is your feed forward neural networkthat we talked about okay so feedforward neural network then you againhave a residual block and then you havelayer normalization and this is oneTransformer block you have multiple ofthem like 11 okay so that Annex layerthat I was referring to is one block soyou have repeated blocks and in the endyou get this kind of soft Maxprobability see the probability here isa station okay so just play with thisparticular Tool uh to get a betterunderstanding of this thing and I wantto give credits uh to this amazingChannel called 3 blue one brown so ifyou go to YouTube and type inTransformer explain 3 blue one brown youwill find all these videos so I want youto watch from video number 5678 onwardshe will have more videos as well uhespecially these three video dl5 dl6 dl7these three videos you must watch itwill enhance your understanding furtherI myself learned a lot from this channelso due credits to three blue one brownall right that's it folks so that's thatwas about Transformer I know it was along discussion there were many topicsthat we covered but hopefully yourunderstanding is clear if you have anyquestion please feel free to ask[Music]a[Music]\""
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transcript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnDvMbO8GsHa"
      },
      "source": [
        "## Step1b: Indexing(Text Splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rda19W5hGPlo"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6--5I8wGfcr"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFDNnqR1HJYZ"
      },
      "outputs": [],
      "source": [
        "chunks = text_splitter.create_documents([transcript])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBexWGusHLNT",
        "outputId": "252f5f5a-c50d-4bcc-f9a5-9f9533ce9704"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk5HctDXHbjV",
        "outputId": "89d7f8a8-83d0-4cb1-fe1e-bb4fdc4ab57f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content=\"Chad GPT is powered by a model calledGPT which is based on a deep learningarchitecture called TransformersTransformers is the reason behind modernday AI boom as an AI Enthusiast when youstart learning Transformers you willcome across this complex diagram whichwill start giving you a headacheimmediately my goal for today's video isto explain you Transformers in a mostsimplified and intuitive manner we needto cover many different topics so thisis going to be a long video attentionand patience is all I need from youtoday when you type in a sentence inGmail it tries to predict next word ornext set of words this is possiblebecause of a machine learning modelcalled language model Google for examplehas this popular language model calledbird which is powering hundreds of AIapplications throughout the world GPTwhich is a model behind chat GPT is alarge language model the reason it iscalled large language model is becauseit has billions of parameters it is muchmore capable and Powerful compared tobird and it is trained on humongousamount of data fundamentally though itis also doing the same thing which iswhen you type in a question in chat GPTit will predict the next word in thatsentence and then it will take theoriginal question and the next predictedword as an input and then predict thenext word and then the next word and soon in the end it produces is a completeanswer which almost sounds like a magicto summarize the goal of a languagemodel is to predict a next word in asentence now\")"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0fquWutJ_NL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PtPFEbZKaLH"
      },
      "source": [
        "## Step 1c: Indexing(Vector Store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh99qbW9KeaG"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODYyzoF2Kxqn"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498,
          "referenced_widgets": [
            "a21083721480483488f565f885fca559",
            "ad00e43f6e2649d98235a41d52b9d6e6",
            "2325b12f9b3642a79e84c7a2fe0fbcb7",
            "d60ace03e891486d9bb6c999959a789f",
            "ec00d40b71a0470895e92e31ce4afa23",
            "82724547960d4a1e84d5dae382bdf333",
            "d52b9631f3604f64beea14db10cfe56f",
            "1e853db6428245369545fdb5c8a32d7e",
            "266ebb120f0b41a29999610e08bd5d50",
            "c290ec6f2d4a4ad382ff965aa5bb4776",
            "ac927a60b561417bbbb269d97a6302a4",
            "d0708e7d586b4a909460a7212356ffad",
            "03edc535a89745a4ba493688b35293b9",
            "24282af6e82044808c051dc0fcc8e53f",
            "e93c467cd70e4757a78fd381d059ad9f",
            "3cb5ad54902d4ca297c0277d72ce0ef3",
            "41f3c3a24b834532898e83ab49459ab2",
            "9149dc494c144651b3bb3f768ddcefd5",
            "725179a964df48a781acfff2f3ec81e9",
            "f1b422ae12d645e89bb9262031812521",
            "fb89b2760e304964b7b20029606058b3",
            "92e913f6aa684944a6875c1be80adf01",
            "8ed7eb07fb5d4e66a70ac9f4c012db7d",
            "c6d25e74b1b64f1d9451e2bc525f664e",
            "5999461ed9c144a7992c5c4372f96b1a",
            "c0868f77d3b64e5ebaa282cedc9e7e48",
            "761cecf27fdb417ba2c3cc67edc598c1",
            "11d3647662044abfa6b85d008840e6b3",
            "3302d3f7ad24496ebc12320065a75fce",
            "fef9cc760c2c40069d71b5bfa5dd5443",
            "dbedacda1e114a2cab2622a62e94b0c4",
            "95d9a8e446724b3fbe3d1f0d57cc76f8",
            "eb310f39e56d441cb869f94da2c066ad",
            "d8a0f0c9f4cd49d9a87cd83c122b61f3",
            "59b29cf33710445daea2225549acef25",
            "3f05d915901044209a1b9a711194085a",
            "7dc1bb64184a466f94ba9ec5d9e147dc",
            "df80a20cd2fd4c459cc10691779919c6",
            "7373062f49034dc2bd7d60a9cba93772",
            "ab2476bfa2ea436bb6e9180006f4184b",
            "6b8f8134e00b4bc2b8c1240caa264092",
            "4e1465b5271e4ec9a766be0cf52b4982",
            "fccce84947c142b79d02954394b03e06",
            "25a77a670e3f420d9ad94f72eb21b58a",
            "df101f4dd5d44990a9bf7442f3074297",
            "489e699079894c76b298511e39b0f1b3",
            "353b6940c986476597f101deda89944c",
            "47d81250a9d6497bad7d06a1a5eedb1b",
            "6a4268995e2440af900218a5256d8a08",
            "681402df3186405880544dc5ff530b80",
            "cc410112297f4e03a3748e917fa5414b",
            "3952fc5624a445849683befe8502ddbc",
            "e7926ad4f10f44bcb8118252d853d2fd",
            "ce4974854ad9496586e7d4f488cc3963",
            "3d0e2dae7f824dedaba55ac8bb8ff5e8",
            "53ba66be413f4652b87c93710263a080",
            "9e4743409f174a6eb49055357e5ce80f",
            "f7585d88a77741ebbfb56352a2108fde",
            "426d2126583f465fa8023cd5e4068a81",
            "23a74d2d7a3041ef9746accfd6b3a4db",
            "b537a4527cfa4d5881dbc334ba46fb5f",
            "5aae518c17f74e0d99d4c9fea6c12ac8",
            "62ef14d70d874bb79922c00e93c85d05",
            "488271fe92c844968b50ada324ed9a60",
            "3edb07bb22264b07ba8de244461de305",
            "92cd4914f80a4792bdc53d2a19c94676",
            "cc7bf8f29acc450995be812ef7daf0b2",
            "8b83d957fe5b43d298d047bb66a03fe6",
            "29b52c434a2443c3bfa16ec7a5477c8f",
            "80d81dd26f374b9f89ed4ad1277bed30",
            "28772262c8884f9a9d3cba914726c1bf",
            "fecc150d3a2d4f7184ee1deb79c0c569",
            "c66f0e35a5154a72b211fe48129c119c",
            "73c363d49c8b4f909c3d29fc3a5e99d1",
            "1cf570afd9de456eba521fb631332d3b",
            "d74d1b2011eb4551ade473cb19bec260",
            "780aca304e7a44c19969951ab8e40280",
            "d62308f48db4431cabc8fe1267508e74",
            "352d43ea54d2493e9dae65b495af84b3",
            "bccf35b79efd423b8aa97eba9a861c63",
            "cc03d2aea93a4e4683589e095ccb8bb3",
            "a2c611f0410444ec87f44c12c936f771",
            "f72d94a079e64da2924bd2aa8a8449c7",
            "237166023d30439094b42815462af63e",
            "0da7808b2de74d4095e3563ccc69146d",
            "0b9c955172474da99e44117545c8eead",
            "05426a7b77cf40d4a510daf289d14f21",
            "7116c342fa694d9dabeb9af5bd8e3c62",
            "fa28e2bdd58142d999ff504178474a16",
            "c70865e13b9e4ae7b6f7e6483370d63a",
            "894117d55abb424ba81341c09396381f",
            "03d1ef3abb5c4547bb640a6a2d263e5f",
            "9bcff9030a4e414e9ff5b04276d36d30",
            "42f44c1c9e554f2eab4736e4bdf1679e",
            "420fd067d39246a7864e1577f57cf7c6",
            "380e5d9aa05a4a0a9efb8a617475a1cc",
            "4150113fbb524a73830b5a28a9e7d581",
            "64ed5565a19c44d980323135ca6dc279",
            "33fa891251134c05b780e6e50088a206",
            "a3a37ca5b8dd482c96def0f86b9b7c78",
            "6cfba888d6ef4ccf954e726985e2ea02",
            "60d8ba0a0f184c8a9991afa3735f2964",
            "e61e147c7dda4c079ae98ccaf47c92ce",
            "c2247ac6e48348fe831550619ad75e61",
            "4aabc93a29b1460995af9ce7759d85b4",
            "13dce4eeff9b49b88004016c2f685edc",
            "5bc0cb920ad242ca9f0087d62d0b9713",
            "fd6118cf767a437db9bb91584ba1e294",
            "f5d69a3616a14eb98db071bf9b584bb2",
            "819d655d3b0247008177a12ce58e396f",
            "c0ba8e7776a04d0c890f2ff74425c3bd",
            "c3ea8df0b88c431ab2d1f9b5af75d2f0",
            "986309e7eef641359bf308efc2e02e73",
            "c922509777154d4eb74f56bfe8e14f79",
            "d4a2fc8a502b4525a1f0fe64c91f4a07",
            "24617d2ef7db4b2287b7d2499c1f1d3c",
            "a31c31bf9c5f4d989a0237579cfaefc2",
            "862c1f5e1fbb4d56add28e6806027a69",
            "4a12dec11f1f4eadbbc1d9c613a71e54",
            "6341f89b697d4487b5233fff40665646",
            "40964214771344d8847716971d010c03"
          ]
        },
        "id": "Z6rkei34Kxwd",
        "outputId": "dfc8062f-8182-4df3-876c-043844d0df31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a21083721480483488f565f885fca559",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0708e7d586b4a909460a7212356ffad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ed7eb07fb5d4e66a70ac9f4c012db7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8a0f0c9f4cd49d9a87cd83c122b61f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df101f4dd5d44990a9bf7442f3074297",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53ba66be413f4652b87c93710263a080",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc7bf8f29acc450995be812ef7daf0b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d62308f48db4431cabc8fe1267508e74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa28e2bdd58142d999ff504178474a16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3a37ca5b8dd482c96def0f86b9b7c78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0ba8e7776a04d0c890f2ff74425c3bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIb3_q6mKhxu"
      },
      "outputs": [],
      "source": [
        "vector_store = FAISS.from_documents(chunks,embedding=embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5LWHR3_Ly4c",
        "outputId": "31e6987d-64e2-4bb3-8edc-c9501ced9cba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'e87af6b5-101d-41c3-8586-89ed3ad082d9',\n",
              " 1: '29aa4005-1c30-448c-af95-67bdd2a60278',\n",
              " 2: '4fe9b001-439d-4012-9da4-2e0f6dfcf922',\n",
              " 3: 'a90c61d3-30f3-4228-85cb-be7e754834c5',\n",
              " 4: '64924563-69d2-4765-a860-33da32949329',\n",
              " 5: '88b2cba4-d4d6-4c12-8559-166223a5d8b9',\n",
              " 6: 'bb3904f8-96b4-4890-a32c-619f9e5a7045',\n",
              " 7: '0fb632e9-e82a-42ed-b852-4035880e8165',\n",
              " 8: 'b6a1f4b0-00ec-4292-8a55-24aa8f1d481f',\n",
              " 9: 'a58394cf-a95d-43e4-9cba-33c94af8a9ba',\n",
              " 10: '42f23d7e-3353-4567-88dd-ed36daea0e7e',\n",
              " 11: '02de8f54-0649-4e9d-a44b-d5b0239af692',\n",
              " 12: '4e760ba4-bd2d-40d8-814b-95ca3a9ab166',\n",
              " 13: '9033a77d-00cc-40db-a90b-fe84edd41335',\n",
              " 14: '9947c102-b66f-477b-a4a6-7aa1b8a7e35a',\n",
              " 15: '877eb898-462c-4468-b727-5450b80876b9',\n",
              " 16: 'b1b765aa-e19c-496a-808e-c3e202c016e2',\n",
              " 17: '3dc3f6b4-e041-4e6f-aaf9-d85833e7de27',\n",
              " 18: '0f876bf6-2859-437a-b017-b2f9f7c9c48e',\n",
              " 19: '098c969f-9e8f-4e9d-9f97-11616d8e1b5c',\n",
              " 20: '31279875-5d64-4892-8e02-85795846e217',\n",
              " 21: '36e6d820-7c28-4b1c-ac9e-6162fdd9f473',\n",
              " 22: '17d6caa0-c168-4d3c-8a01-b919f1994535',\n",
              " 23: '39e4aead-302d-4490-8762-9a62d4c33951',\n",
              " 24: '195c9b21-94c6-4269-b796-4cb3cf0ff3ae',\n",
              " 25: '6bc00fea-fd07-4d4c-8c91-9f0ae6b8f287',\n",
              " 26: '43c0bd80-479f-47c8-9d8a-c75d6e9e2ba4',\n",
              " 27: '8414a163-a140-4c4e-ba87-1df9b1731001',\n",
              " 28: '0b91a87c-bf8b-4c7b-921d-ead5500d7fb8',\n",
              " 29: '28f0490f-72a5-420f-a584-1ba0a7cba6aa',\n",
              " 30: '07024a8e-6f98-4098-ba7f-689fdea7c9c0',\n",
              " 31: 'dd8f637b-ba33-4c6c-934a-0d088003e793',\n",
              " 32: '96fbdae6-3731-4638-9ebd-b227a761144d',\n",
              " 33: '2eec05e6-7b22-4847-9435-d63f72df823d',\n",
              " 34: '4bf906a5-e7ed-451a-b3cd-4e35287bbd1f',\n",
              " 35: '6090e73d-4db4-42be-88e5-e82a355f6152'}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.index_to_docstore_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSSpGxk_L4n-",
        "outputId": "8029844a-bc55-4a68-91b7-edff6a688c8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='2eec05e6-7b22-4847-9435-d63f72df823d', metadata={}, page_content=\"this is the arrow whereas Q iscoming from the decoder itself right soyou know that Hindi sequence will beproduced here so man K banai Etc so thatquery part is coming from here that iswhy it is called cross attention in thecase of B we all know that there is onlyencoder part decoder part is not therein case of GPT the Transformerarchitecture is little different okay sothat's the encoder part remaining partwe have already understood now let me uhshow one nice tool which can visuallyshow you this architecture someone hasbuilt this nice visualization tool youcan go to Pol club. github.ioTransformer explainer and here you canlook into different examples right sofor example let's look at this sentenceas the space ship was approaching the itwill try to autocomplete that word andsay station right now each of thesewords has the space ship first you hadthis Dropout layer so we talking aboutTransformer here so it will have aDropout layer the architecture is littlecustomized compared to the baseTransformer architecture then you havethis residual connection okay soresidual connection will take you fromhere to here if you talk aboutembeddings you have token embeddings foreach of these right see 768 is the sizethen you have positional embedding okayyou add all this position and you getfinal Vector this is apositional embedding of a sentence andyou have residual connection then youhave q KV computation so if you look atthis particular block here qkv it willkind of visually show you how\")]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.get_by_ids(['2eec05e6-7b22-4847-9435-d63f72df823d'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to91I-FwMBiK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_VeWz0QMPuu"
      },
      "source": [
        "## Step 2: Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMwE76tjMSJ7"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_type = 'similarity',search_kwargs = {'k':5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-YAuT-6Mc-m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGEk8cRgM_yr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmAIVV6UNBVK"
      },
      "source": [
        "## Step 3 : Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G24OGmMMivL"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "# Store token in environment\n",
        "\n",
        "\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    task=\"text-generation\",\n",
        ")\n",
        "chat_model  = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKRuvdW6M8s-"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtwwbXR4NJJr"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template =\"\"\"\n",
        "    You are a helpful assistent.\n",
        "    Answer Only from the provedid context.\n",
        "    If the context is insufficient, just say you don't know.\n",
        "    {context}\n",
        "    Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables=['context','question']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "irvm-SwlNuST",
        "outputId": "81562540-0f58-4a44-aa7d-e3c2fec23961"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'retriever' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3492623099.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Explain what 'for i in range(5): print(i)' does in Python.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mretrieved_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
          ]
        }
      ],
      "source": [
        "question = \"What is bird\"\n",
        "retrieved_docs = retriever.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jhUo-Q_OQJn"
      },
      "outputs": [],
      "source": [
        "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "yTSvKdQ0gNI_",
        "outputId": "564912b0-5ee0-43d2-85d1-29de44363463"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"whenever youhave these uh deep learning model youhave two stages one stage is the modelis not trained it's like a baby baby isnot trained yet and you train them rightyou send them to school you train themuh in your home at some point theybecome adult and they can figure thingsout on their own similarly a machinelearning model goes through a trainingphase and when it is ready it startsworking on the real world problem andthat is called inference so whatever Italked about for for predicting the nextword or translating sentence I wasreferring to inference stage throughoutthe discussion we'll be referring to twospecific models called bird and GPT ifyou look at this architecture that's ageneric architecture for a Transformermodel Transformer model is a generalconcept whereas bird and GPT arespecific model or specificimplementations based on Transformerarchitecture if you look at birdarchitecture it has only the encoderpart okay so only this part decoder partis missing so but will take the inputsentence it will produce the contextualembedding and that's it whereas GPT hasonly decoder so it still takes the inputit will produce the contextual embeddingand so on and then here it will predictthe next word I mean it sounds like ithas encoder decoder both butfundamentally the architecture lookslittle different for GPT but it is stillbased on the Transformer architecturethe way they're trained is you take allthe text from Wikipedia crawl text frominternet book Corpus and you train\\n\\na listmade word is at position2532 if you talk about bird it has total30,00 , 522 tokens and GPT has around50,000 tokens from these token ID sostep number one was generate token andtoken IDs then you uh get the staticembedding for each of these tokens andfrom where do you get it well we justsaw right during the training you aregenerating this static word embeddingmetrix so for each of the words ortokens you havethe static word embedding so in the caseof bird the size of this will be 768 ifit is GPT it will be 12,000 you knowthat long embedding metrics so youproduce that for each of the tokens andthen you will also create somethingcalled a positional embedding now in thelanguage the word order matters okay soif I put made before I it will changethe meaning of that sentence so theorder matters and the way Transformerworks is it will process the entireinput or sequence in parallel it is notlike RNN where it will process thesewords one by one it will process thissequence All In Parallel now it needs tohave knowledge on the order okay so forthat it uses a special technique calledpositional embedding where it will add asmall Vector in each of these embeddingsokay so let's say this is the vector forposition number one this is the vectorfor position number two and so on andwhen you get uh this resulting Vectorthis Vector will embed the knowledge ofposition so this Vector will have aknowledge that this is the first wordthis Vector will have a knowledge thatthis is the second word now\\n\\nChad GPT is powered by a model calledGPT which is based on a deep learningarchitecture called TransformersTransformers is the reason behind modernday AI boom as an AI Enthusiast when youstart learning Transformers you willcome across this complex diagram whichwill start giving you a headacheimmediately my goal for today's video isto explain you Transformers in a mostsimplified and intuitive manner we needto cover many different topics so thisis going to be a long video attentionand patience is all I need from youtoday when you type in a sentence inGmail it tries to predict next word ornext set of words this is possiblebecause of a machine learning modelcalled language model Google for examplehas this popular language model calledbird which is powering hundreds of AIapplications throughout the world GPTwhich is a model behind chat GPT is alarge language model the reason it iscalled large language model is becauseit has billions of parameters it is muchmore capable and Powerful compared tobird and it is trained on humongousamount of data fundamentally though itis also doing the same thing which iswhen you type in a question in chat GPTit will predict the next word in thatsentence and then it will take theoriginal question and the next predictedword as an input and then predict thenext word and then the next word and soon in the end it produces is a completeanswer which almost sounds like a magicto summarize the goal of a languagemodel is to predict a next word in asentence now\\n\\na vertical format this is horizontalformat the same thing for each of thesewords or tokens you will first get theirembedding from our stating embeddingMatrix okay so these are staticembeddings for each of these words inthe case of bird the dimension is 768for GPT is 12,000 something let's sayfor word dish this is my embedding let'scall it E7 that E7 you will multiplywith a special Matrix called WQ whichwill have adimension uh of 64 by 768 so 768 is thecolumns in order to perform matrixmultiplication The Columns in the firstMatrix should be equal to rows in thesecond Matrix so this is 6 768 this is768 the rows in The Matrix the firstMatrix is 64 for bir for GPT isdifferent and when you douh this kind of matrix multiplicationyou will get uh this quy Vector okay soyou will multiply this row with thiscolumn okay so you multiply 50 with this0.9minus5 with1.07 65 with this and then you add themall up you put them here then you takethe second row multiply 23 with thisminus 71 with this 1.58 with this andyou put that here and so on okay so thisis how you build a query Vector now WQhere knows how to encode query of atoken for attention computation when wetrain the model we already got the WQand WQ after the training is done it itdoesn't change okay after you do thattraining sometime it is referred aspre-training on huge amount of data youbuild this WQ Matrix which doesn'tchange okay so for a train model uh thisWQ will not change you multiply thatwith specific embedding E7 let's\\n\\nthat we can can represent bunch ofwords such as battle horse King and soon by asking set of questions okay sofor battle they don't have authority arethey an event yes do battle has tail nobattle is an event it it doesn't havetail and so on similarly horse do theyhave authority well we'll just say 01 ifit is King's horse maybe they have someAuthority or maybe they have authorityover their horse kids and so onsimilarly we can represent all thesewords in a numeric format and then wecan take the vector of this word Kingand maybe we can do some mathematicswith it we can say King minus man sohere I'm taking the vector of man rightwhich is uh this particular Vector pluswoman which is this particular vectorand when you do the math which is like 1- 2 + 2 will beZ and so on you get a vector which lookssimilar to Queen now this sounds like amagic we can do math with Words which isKing minus man plus woman is equal toQueen here King was represented in fiveDimensions when you look at the reallife for example Google's word to wackmodel it has 300 dimensions and what areall these questions by the way well weactually don't know this has beentrained through a neural network and wehave processed huge amount of text suchas all the Wikipedia articles all thebooks and text on internet to understandthe relationship between these words andthrough that neural network trainingback propagation we came up with thisVector the example that I gave for Kingwhere we asked these questions Authorityand so on\""
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X-PvmdgN-n-"
      },
      "outputs": [],
      "source": [
        "final_prompt = prompt.invoke({'context':context_text,'question':question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiV0hXoDOLgY",
        "outputId": "20f9deef-a9b9-4d6f-ebc2-6741dd8787d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StringPromptValue(text=\"\\n    You are a helpful assistent.\\n    Answer Only from the provedid context.\\n    If the context is insufficient, just say you don't know.\\n    whenever youhave these uh deep learning model youhave two stages one stage is the modelis not trained it's like a baby baby isnot trained yet and you train them rightyou send them to school you train themuh in your home at some point theybecome adult and they can figure thingsout on their own similarly a machinelearning model goes through a trainingphase and when it is ready it startsworking on the real world problem andthat is called inference so whatever Italked about for for predicting the nextword or translating sentence I wasreferring to inference stage throughoutthe discussion we'll be referring to twospecific models called bird and GPT ifyou look at this architecture that's ageneric architecture for a Transformermodel Transformer model is a generalconcept whereas bird and GPT arespecific model or specificimplementations based on Transformerarchitecture if you look at birdarchitecture it has only the encoderpart okay so only this part decoder partis missing so but will take the inputsentence it will produce the contextualembedding and that's it whereas GPT hasonly decoder so it still takes the inputit will produce the contextual embeddingand so on and then here it will predictthe next word I mean it sounds like ithas encoder decoder both butfundamentally the architecture lookslittle different for GPT but it is stillbased on the Transformer architecturethe way they're trained is you take allthe text from Wikipedia crawl text frominternet book Corpus and you train\\n\\na listmade word is at position2532 if you talk about bird it has total30,00 , 522 tokens and GPT has around50,000 tokens from these token ID sostep number one was generate token andtoken IDs then you uh get the staticembedding for each of these tokens andfrom where do you get it well we justsaw right during the training you aregenerating this static word embeddingmetrix so for each of the words ortokens you havethe static word embedding so in the caseof bird the size of this will be 768 ifit is GPT it will be 12,000 you knowthat long embedding metrics so youproduce that for each of the tokens andthen you will also create somethingcalled a positional embedding now in thelanguage the word order matters okay soif I put made before I it will changethe meaning of that sentence so theorder matters and the way Transformerworks is it will process the entireinput or sequence in parallel it is notlike RNN where it will process thesewords one by one it will process thissequence All In Parallel now it needs tohave knowledge on the order okay so forthat it uses a special technique calledpositional embedding where it will add asmall Vector in each of these embeddingsokay so let's say this is the vector forposition number one this is the vectorfor position number two and so on andwhen you get uh this resulting Vectorthis Vector will embed the knowledge ofposition so this Vector will have aknowledge that this is the first wordthis Vector will have a knowledge thatthis is the second word now\\n\\nChad GPT is powered by a model calledGPT which is based on a deep learningarchitecture called TransformersTransformers is the reason behind modernday AI boom as an AI Enthusiast when youstart learning Transformers you willcome across this complex diagram whichwill start giving you a headacheimmediately my goal for today's video isto explain you Transformers in a mostsimplified and intuitive manner we needto cover many different topics so thisis going to be a long video attentionand patience is all I need from youtoday when you type in a sentence inGmail it tries to predict next word ornext set of words this is possiblebecause of a machine learning modelcalled language model Google for examplehas this popular language model calledbird which is powering hundreds of AIapplications throughout the world GPTwhich is a model behind chat GPT is alarge language model the reason it iscalled large language model is becauseit has billions of parameters it is muchmore capable and Powerful compared tobird and it is trained on humongousamount of data fundamentally though itis also doing the same thing which iswhen you type in a question in chat GPTit will predict the next word in thatsentence and then it will take theoriginal question and the next predictedword as an input and then predict thenext word and then the next word and soon in the end it produces is a completeanswer which almost sounds like a magicto summarize the goal of a languagemodel is to predict a next word in asentence now\\n\\na vertical format this is horizontalformat the same thing for each of thesewords or tokens you will first get theirembedding from our stating embeddingMatrix okay so these are staticembeddings for each of these words inthe case of bird the dimension is 768for GPT is 12,000 something let's sayfor word dish this is my embedding let'scall it E7 that E7 you will multiplywith a special Matrix called WQ whichwill have adimension uh of 64 by 768 so 768 is thecolumns in order to perform matrixmultiplication The Columns in the firstMatrix should be equal to rows in thesecond Matrix so this is 6 768 this is768 the rows in The Matrix the firstMatrix is 64 for bir for GPT isdifferent and when you douh this kind of matrix multiplicationyou will get uh this quy Vector okay soyou will multiply this row with thiscolumn okay so you multiply 50 with this0.9minus5 with1.07 65 with this and then you add themall up you put them here then you takethe second row multiply 23 with thisminus 71 with this 1.58 with this andyou put that here and so on okay so thisis how you build a query Vector now WQhere knows how to encode query of atoken for attention computation when wetrain the model we already got the WQand WQ after the training is done it itdoesn't change okay after you do thattraining sometime it is referred aspre-training on huge amount of data youbuild this WQ Matrix which doesn'tchange okay so for a train model uh thisWQ will not change you multiply thatwith specific embedding E7 let's\\n\\nthat we can can represent bunch ofwords such as battle horse King and soon by asking set of questions okay sofor battle they don't have authority arethey an event yes do battle has tail nobattle is an event it it doesn't havetail and so on similarly horse do theyhave authority well we'll just say 01 ifit is King's horse maybe they have someAuthority or maybe they have authorityover their horse kids and so onsimilarly we can represent all thesewords in a numeric format and then wecan take the vector of this word Kingand maybe we can do some mathematicswith it we can say King minus man sohere I'm taking the vector of man rightwhich is uh this particular Vector pluswoman which is this particular vectorand when you do the math which is like 1- 2 + 2 will beZ and so on you get a vector which lookssimilar to Queen now this sounds like amagic we can do math with Words which isKing minus man plus woman is equal toQueen here King was represented in fiveDimensions when you look at the reallife for example Google's word to wackmodel it has 300 dimensions and what areall these questions by the way well weactually don't know this has beentrained through a neural network and wehave processed huge amount of text suchas all the Wikipedia articles all thebooks and text on internet to understandthe relationship between these words andthrough that neural network trainingback propagation we came up with thisVector the example that I gave for Kingwhere we asked these questions Authorityand so on\\n    Question: What is bird\\n    \")"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCJ6xRHxPv0O"
      },
      "source": [
        "## Step 4: Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaPj-8lpP1eq",
        "outputId": "019f3cf6-d300-4c16-ab7d-9cf8e4427b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bird is a machine learning model powered by a language model. It has a total of 30,000 tokens and is used to predict the next word in a sentence.\n"
          ]
        }
      ],
      "source": [
        "final_result = chat_model.invoke(final_prompt)\n",
        "print(final_result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQPd9wxHP9t3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zkR9fGJRDHr"
      },
      "source": [
        "## Building A Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZqWuTwfRFkA"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6MeYqwlSq7z"
      },
      "outputs": [],
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdhUp87IWgL2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "2F9vauXlWhC-",
        "outputId": "e1dcf7da-d82e-4110-9d6d-e14eccd8e969"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"whenever youhave these uh deep learning model youhave two stages one stage is the modelis not trained it's like a baby baby isnot trained yet and you train them rightyou send them to school you train themuh in your home at some point theybecome adult and they can figure thingsout on their own similarly a machinelearning model goes through a trainingphase and when it is ready it startsworking on the real world problem andthat is called inference so whatever Italked about for for predicting the nextword or translating sentence I wasreferring to inference stage throughoutthe discussion we'll be referring to twospecific models called bird and GPT ifyou look at this architecture that's ageneric architecture for a Transformermodel Transformer model is a generalconcept whereas bird and GPT arespecific model or specificimplementations based on Transformerarchitecture if you look at birdarchitecture it has only the encoderpart okay so only this part decoder partis missing so but will take the inputsentence it will produce the contextualembedding and that's it whereas GPT hasonly decoder so it still takes the inputit will produce the contextual embeddingand so on and then here it will predictthe next word I mean it sounds like ithas encoder decoder both butfundamentally the architecture lookslittle different for GPT but it is stillbased on the Transformer architecturethe way they're trained is you take allthe text from Wikipedia crawl text frominternet book Corpus and you train\\n\\na listmade word is at position2532 if you talk about bird it has total30,00 , 522 tokens and GPT has around50,000 tokens from these token ID sostep number one was generate token andtoken IDs then you uh get the staticembedding for each of these tokens andfrom where do you get it well we justsaw right during the training you aregenerating this static word embeddingmetrix so for each of the words ortokens you havethe static word embedding so in the caseof bird the size of this will be 768 ifit is GPT it will be 12,000 you knowthat long embedding metrics so youproduce that for each of the tokens andthen you will also create somethingcalled a positional embedding now in thelanguage the word order matters okay soif I put made before I it will changethe meaning of that sentence so theorder matters and the way Transformerworks is it will process the entireinput or sequence in parallel it is notlike RNN where it will process thesewords one by one it will process thissequence All In Parallel now it needs tohave knowledge on the order okay so forthat it uses a special technique calledpositional embedding where it will add asmall Vector in each of these embeddingsokay so let's say this is the vector forposition number one this is the vectorfor position number two and so on andwhen you get uh this resulting Vectorthis Vector will embed the knowledge ofposition so this Vector will have aknowledge that this is the first wordthis Vector will have a knowledge thatthis is the second word now\\n\\nChad GPT is powered by a model calledGPT which is based on a deep learningarchitecture called TransformersTransformers is the reason behind modernday AI boom as an AI Enthusiast when youstart learning Transformers you willcome across this complex diagram whichwill start giving you a headacheimmediately my goal for today's video isto explain you Transformers in a mostsimplified and intuitive manner we needto cover many different topics so thisis going to be a long video attentionand patience is all I need from youtoday when you type in a sentence inGmail it tries to predict next word ornext set of words this is possiblebecause of a machine learning modelcalled language model Google for examplehas this popular language model calledbird which is powering hundreds of AIapplications throughout the world GPTwhich is a model behind chat GPT is alarge language model the reason it iscalled large language model is becauseit has billions of parameters it is muchmore capable and Powerful compared tobird and it is trained on humongousamount of data fundamentally though itis also doing the same thing which iswhen you type in a question in chat GPTit will predict the next word in thatsentence and then it will take theoriginal question and the next predictedword as an input and then predict thenext word and then the next word and soon in the end it produces is a completeanswer which almost sounds like a magicto summarize the goal of a languagemodel is to predict a next word in asentence now\\n\\na vertical format this is horizontalformat the same thing for each of thesewords or tokens you will first get theirembedding from our stating embeddingMatrix okay so these are staticembeddings for each of these words inthe case of bird the dimension is 768for GPT is 12,000 something let's sayfor word dish this is my embedding let'scall it E7 that E7 you will multiplywith a special Matrix called WQ whichwill have adimension uh of 64 by 768 so 768 is thecolumns in order to perform matrixmultiplication The Columns in the firstMatrix should be equal to rows in thesecond Matrix so this is 6 768 this is768 the rows in The Matrix the firstMatrix is 64 for bir for GPT isdifferent and when you douh this kind of matrix multiplicationyou will get uh this quy Vector okay soyou will multiply this row with thiscolumn okay so you multiply 50 with this0.9minus5 with1.07 65 with this and then you add themall up you put them here then you takethe second row multiply 23 with thisminus 71 with this 1.58 with this andyou put that here and so on okay so thisis how you build a query Vector now WQhere knows how to encode query of atoken for attention computation when wetrain the model we already got the WQand WQ after the training is done it itdoesn't change okay after you do thattraining sometime it is referred aspre-training on huge amount of data youbuild this WQ Matrix which doesn'tchange okay so for a train model uh thisWQ will not change you multiply thatwith specific embedding E7 let's\\n\\nthat we can can represent bunch ofwords such as battle horse King and soon by asking set of questions okay sofor battle they don't have authority arethey an event yes do battle has tail nobattle is an event it it doesn't havetail and so on similarly horse do theyhave authority well we'll just say 01 ifit is King's horse maybe they have someAuthority or maybe they have authorityover their horse kids and so onsimilarly we can represent all thesewords in a numeric format and then wecan take the vector of this word Kingand maybe we can do some mathematicswith it we can say King minus man sohere I'm taking the vector of man rightwhich is uh this particular Vector pluswoman which is this particular vectorand when you do the math which is like 1- 2 + 2 will beZ and so on you get a vector which lookssimilar to Queen now this sounds like amagic we can do math with Words which isKing minus man plus woman is equal toQueen here King was represented in fiveDimensions when you look at the reallife for example Google's word to wackmodel it has 300 dimensions and what areall these questions by the way well weactually don't know this has beentrained through a neural network and wehave processed huge amount of text suchas all the Wikipedia articles all thebooks and text on internet to understandthe relationship between these words andthrough that neural network trainingback propagation we came up with thisVector the example that I gave for Kingwhere we asked these questions Authorityand so on\""
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "format_docs(retrieved_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20PYLTGhR4s3"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'context': itemgetter(\"question\") | retriever | RunnableLambda(format_docs),\n",
        "    'question': itemgetter(\"question\")\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlZx833YS_4F",
        "outputId": "efcf82c4-7141-4cdd-a52c-71945321350b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': \"lookslittle different for GPT but it is stillbased on the Transformer architecturethe way they're trained is you take allthe text from Wikipedia crawl text frominternet book Corpus and you train thesemodels when you have this article forexample and you are having this sentencedeveloping an advanced crude see if Igive you this sentence most likely youwill say spacecraft or vehicle as a nextword you'll not say banana right like soprobability of having banana as a nextword in this sentence is very lesswhereas these two words have a highprobability so we as humans have read somuch text so now we have learned thisart of predicting next word and samething goes on for B and GPT where theyunderstand the relationship between thewords the context in which they appearso let's say if B during the traininghas encountered so much text and everytime after this word crude if it hasseen this word spacecraft of or vehicleuh it would not have seen words likecrude chair or crude banana right thatkind of words usually when it is goingthrough training it will not see it soit will learn to predict the highprobability worse right so for word likebanana probability is going to be lowersame thing for this article when you gothrough this kind of sentences right SIengaging both alliances and hostilitiesand there will be many more artic onbattles and Warriors and everywhereafter alliances there will be eitherhostilities or negotiations there willnot be a word like chair so probabilityof that will be\\n\\nChad GPT is powered by a model calledGPT which is based on a deep learningarchitecture called TransformersTransformers is the reason behind modernday AI boom as an AI Enthusiast when youstart learning Transformers you willcome across this complex diagram whichwill start giving you a headacheimmediately my goal for today's video isto explain you Transformers in a mostsimplified and intuitive manner we needto cover many different topics so thisis going to be a long video attentionand patience is all I need from youtoday when you type in a sentence inGmail it tries to predict next word ornext set of words this is possiblebecause of a machine learning modelcalled language model Google for examplehas this popular language model calledbird which is powering hundreds of AIapplications throughout the world GPTwhich is a model behind chat GPT is alarge language model the reason it iscalled large language model is becauseit has billions of parameters it is muchmore capable and Powerful compared tobird and it is trained on humongousamount of data fundamentally though itis also doing the same thing which iswhen you type in a question in chat GPTit will predict the next word in thatsentence and then it will take theoriginal question and the next predictedword as an input and then predict thenext word and then the next word and soon in the end it produces is a completeanswer which almost sounds like a magicto summarize the goal of a languagemodel is to predict a next word in asentence now\\n\\nwhenever youhave these uh deep learning model youhave two stages one stage is the modelis not trained it's like a baby baby isnot trained yet and you train them rightyou send them to school you train themuh in your home at some point theybecome adult and they can figure thingsout on their own similarly a machinelearning model goes through a trainingphase and when it is ready it startsworking on the real world problem andthat is called inference so whatever Italked about for for predicting the nextword or translating sentence I wasreferring to inference stage throughoutthe discussion we'll be referring to twospecific models called bird and GPT ifyou look at this architecture that's ageneric architecture for a Transformermodel Transformer model is a generalconcept whereas bird and GPT arespecific model or specificimplementations based on Transformerarchitecture if you look at birdarchitecture it has only the encoderpart okay so only this part decoder partis missing so but will take the inputsentence it will produce the contextualembedding and that's it whereas GPT hasonly decoder so it still takes the inputit will produce the contextual embeddingand so on and then here it will predictthe next word I mean it sounds like ithas encoder decoder both butfundamentally the architecture lookslittle different for GPT but it is stillbased on the Transformer architecturethe way they're trained is you take allthe text from Wikipedia crawl text frominternet book Corpus and you train\\n\\norderbut you essentially add all the vectorsokay so you just add all of thiseverything okay so here I'm not showingeverything but you kind of get an ideaso from static embedding you go all theway to context aware embedding here'sthe mathematical formula for attentionqk V where DK is a dimension of a keyvectorin case of GPT this is 128 so what theydo is they take um the entire 12228 Dimension right for GPT thedimension of the contextual embeddingsis 12 228 and you divide it by thenumber of attention heads I think forGPT is 96 and that's how you get 128 Iwill explain this 96 a little later butthere is a way to derive this number 128so you do division by square root ofthat just for numerical stability youdon't want this dot products to becomevery high okay so to bring down thatnumber we do kind of scaling here andyou do soft Max and you multiply thatwith this value V so far what we talkedabout is a single attention blockactually there are multiple attentionblocks so that's what we'll cover nextlet's understand what is multi headattention so far we have seen thispicture where you take positionalembedding for each of the words in yourinput sequence you let it go throughattention head which is basically takingthis WQ WKWV and coming up with context our awareembedding so that whole portion iscalled one attention head in reality youhave multiple attention heads okay soyou have multiple attention heads eachof these heads are producing their owncontext aware embedding which you\\n\\na listmade word is at position2532 if you talk about bird it has total30,00 , 522 tokens and GPT has around50,000 tokens from these token ID sostep number one was generate token andtoken IDs then you uh get the staticembedding for each of these tokens andfrom where do you get it well we justsaw right during the training you aregenerating this static word embeddingmetrix so for each of the words ortokens you havethe static word embedding so in the caseof bird the size of this will be 768 ifit is GPT it will be 12,000 you knowthat long embedding metrics so youproduce that for each of the tokens andthen you will also create somethingcalled a positional embedding now in thelanguage the word order matters okay soif I put made before I it will changethe meaning of that sentence so theorder matters and the way Transformerworks is it will process the entireinput or sequence in parallel it is notlike RNN where it will process thesewords one by one it will process thissequence All In Parallel now it needs tohave knowledge on the order okay so forthat it uses a special technique calledpositional embedding where it will add asmall Vector in each of these embeddingsokay so let's say this is the vector forposition number one this is the vectorfor position number two and so on andwhen you get uh this resulting Vectorthis Vector will embed the knowledge ofposition so this Vector will have aknowledge that this is the first wordthis Vector will have a knowledge thatthis is the second word now\",\n",
              " 'question': 'What is GPT'}"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_chain.invoke({\"question\": \"What is GPT\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnQ--9byVQ0i"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAi6VnlEU3py"
      },
      "outputs": [],
      "source": [
        "final_chain = parallel_chain | prompt | chat_model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjGaIRF6VdR_",
        "outputId": "00fef826-a317-400a-d8ff-111717bb4cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT is a large language model that is the model behind the popular chatbot, Chat GPT. It is a deep learning architecture called Transformers that is used for various AI applications. GPT is trained on a massive amount of text data from Wikipedia, internet book corpora, and other sources, which enables it to predict the next word in a sentence and produce coherent and context-aware text.\n"
          ]
        }
      ],
      "source": [
        "result = final_chain.invoke({\"question\": \"What is gpt\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLHCHb5-Vi-7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03d1ef3abb5c4547bb640a6a2d263e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64ed5565a19c44d980323135ca6dc279",
            "placeholder": "​",
            "style": "IPY_MODEL_33fa891251134c05b780e6e50088a206",
            "value": " 466k/? [00:00&lt;00:00, 11.4MB/s]"
          }
        },
        "03edc535a89745a4ba493688b35293b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41f3c3a24b834532898e83ab49459ab2",
            "placeholder": "​",
            "style": "IPY_MODEL_9149dc494c144651b3bb3f768ddcefd5",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "05426a7b77cf40d4a510daf289d14f21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b9c955172474da99e44117545c8eead": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0da7808b2de74d4095e3563ccc69146d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "11d3647662044abfa6b85d008840e6b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13dce4eeff9b49b88004016c2f685edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cf570afd9de456eba521fb631332d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e853db6428245369545fdb5c8a32d7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2325b12f9b3642a79e84c7a2fe0fbcb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e853db6428245369545fdb5c8a32d7e",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_266ebb120f0b41a29999610e08bd5d50",
            "value": 349
          }
        },
        "237166023d30439094b42815462af63e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23a74d2d7a3041ef9746accfd6b3a4db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24282af6e82044808c051dc0fcc8e53f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_725179a964df48a781acfff2f3ec81e9",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1b422ae12d645e89bb9262031812521",
            "value": 116
          }
        },
        "24617d2ef7db4b2287b7d2499c1f1d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25a77a670e3f420d9ad94f72eb21b58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "266ebb120f0b41a29999610e08bd5d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28772262c8884f9a9d3cba914726c1bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29b52c434a2443c3bfa16ec7a5477c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73c363d49c8b4f909c3d29fc3a5e99d1",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cf570afd9de456eba521fb631332d3b",
            "value": 350
          }
        },
        "3302d3f7ad24496ebc12320065a75fce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33fa891251134c05b780e6e50088a206": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "352d43ea54d2493e9dae65b495af84b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f72d94a079e64da2924bd2aa8a8449c7",
            "placeholder": "​",
            "style": "IPY_MODEL_237166023d30439094b42815462af63e",
            "value": "vocab.txt: "
          }
        },
        "353b6940c986476597f101deda89944c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3952fc5624a445849683befe8502ddbc",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7926ad4f10f44bcb8118252d853d2fd",
            "value": 612
          }
        },
        "380e5d9aa05a4a0a9efb8a617475a1cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3952fc5624a445849683befe8502ddbc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb5ad54902d4ca297c0277d72ce0ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d0e2dae7f824dedaba55ac8bb8ff5e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3edb07bb22264b07ba8de244461de305": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f05d915901044209a1b9a711194085a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b8f8134e00b4bc2b8c1240caa264092",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e1465b5271e4ec9a766be0cf52b4982",
            "value": 53
          }
        },
        "40964214771344d8847716971d010c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4150113fbb524a73830b5a28a9e7d581": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41f3c3a24b834532898e83ab49459ab2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "420fd067d39246a7864e1577f57cf7c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "426d2126583f465fa8023cd5e4068a81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3edb07bb22264b07ba8de244461de305",
            "placeholder": "​",
            "style": "IPY_MODEL_92cd4914f80a4792bdc53d2a19c94676",
            "value": " 90.9M/90.9M [00:01&lt;00:00, 103MB/s]"
          }
        },
        "42f44c1c9e554f2eab4736e4bdf1679e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47d81250a9d6497bad7d06a1a5eedb1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce4974854ad9496586e7d4f488cc3963",
            "placeholder": "​",
            "style": "IPY_MODEL_3d0e2dae7f824dedaba55ac8bb8ff5e8",
            "value": " 612/612 [00:00&lt;00:00, 34.7kB/s]"
          }
        },
        "488271fe92c844968b50ada324ed9a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "489e699079894c76b298511e39b0f1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_681402df3186405880544dc5ff530b80",
            "placeholder": "​",
            "style": "IPY_MODEL_cc410112297f4e03a3748e917fa5414b",
            "value": "config.json: 100%"
          }
        },
        "4a12dec11f1f4eadbbc1d9c613a71e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4aabc93a29b1460995af9ce7759d85b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e1465b5271e4ec9a766be0cf52b4982": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53ba66be413f4652b87c93710263a080": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e4743409f174a6eb49055357e5ce80f",
              "IPY_MODEL_f7585d88a77741ebbfb56352a2108fde",
              "IPY_MODEL_426d2126583f465fa8023cd5e4068a81"
            ],
            "layout": "IPY_MODEL_23a74d2d7a3041ef9746accfd6b3a4db"
          }
        },
        "5999461ed9c144a7992c5c4372f96b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fef9cc760c2c40069d71b5bfa5dd5443",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dbedacda1e114a2cab2622a62e94b0c4",
            "value": 1
          }
        },
        "59b29cf33710445daea2225549acef25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7373062f49034dc2bd7d60a9cba93772",
            "placeholder": "​",
            "style": "IPY_MODEL_ab2476bfa2ea436bb6e9180006f4184b",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "5aae518c17f74e0d99d4c9fea6c12ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bc0cb920ad242ca9f0087d62d0b9713": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60d8ba0a0f184c8a9991afa3735f2964": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bc0cb920ad242ca9f0087d62d0b9713",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd6118cf767a437db9bb91584ba1e294",
            "value": 112
          }
        },
        "62ef14d70d874bb79922c00e93c85d05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6341f89b697d4487b5233fff40665646": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64ed5565a19c44d980323135ca6dc279": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "681402df3186405880544dc5ff530b80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a4268995e2440af900218a5256d8a08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b8f8134e00b4bc2b8c1240caa264092": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cfba888d6ef4ccf954e726985e2ea02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aabc93a29b1460995af9ce7759d85b4",
            "placeholder": "​",
            "style": "IPY_MODEL_13dce4eeff9b49b88004016c2f685edc",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "7116c342fa694d9dabeb9af5bd8e3c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "725179a964df48a781acfff2f3ec81e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7373062f49034dc2bd7d60a9cba93772": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73c363d49c8b4f909c3d29fc3a5e99d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "761cecf27fdb417ba2c3cc67edc598c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "780aca304e7a44c19969951ab8e40280": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dc1bb64184a466f94ba9ec5d9e147dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fccce84947c142b79d02954394b03e06",
            "placeholder": "​",
            "style": "IPY_MODEL_25a77a670e3f420d9ad94f72eb21b58a",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.86kB/s]"
          }
        },
        "80d81dd26f374b9f89ed4ad1277bed30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d74d1b2011eb4551ade473cb19bec260",
            "placeholder": "​",
            "style": "IPY_MODEL_780aca304e7a44c19969951ab8e40280",
            "value": " 350/350 [00:00&lt;00:00, 19.2kB/s]"
          }
        },
        "819d655d3b0247008177a12ce58e396f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82724547960d4a1e84d5dae382bdf333": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "862c1f5e1fbb4d56add28e6806027a69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "894117d55abb424ba81341c09396381f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_380e5d9aa05a4a0a9efb8a617475a1cc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4150113fbb524a73830b5a28a9e7d581",
            "value": 1
          }
        },
        "8b83d957fe5b43d298d047bb66a03fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fecc150d3a2d4f7184ee1deb79c0c569",
            "placeholder": "​",
            "style": "IPY_MODEL_c66f0e35a5154a72b211fe48129c119c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8ed7eb07fb5d4e66a70ac9f4c012db7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6d25e74b1b64f1d9451e2bc525f664e",
              "IPY_MODEL_5999461ed9c144a7992c5c4372f96b1a",
              "IPY_MODEL_c0868f77d3b64e5ebaa282cedc9e7e48"
            ],
            "layout": "IPY_MODEL_761cecf27fdb417ba2c3cc67edc598c1"
          }
        },
        "9149dc494c144651b3bb3f768ddcefd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92cd4914f80a4792bdc53d2a19c94676": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92e913f6aa684944a6875c1be80adf01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95d9a8e446724b3fbe3d1f0d57cc76f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "986309e7eef641359bf308efc2e02e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_862c1f5e1fbb4d56add28e6806027a69",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a12dec11f1f4eadbbc1d9c613a71e54",
            "value": 190
          }
        },
        "9bcff9030a4e414e9ff5b04276d36d30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e4743409f174a6eb49055357e5ce80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b537a4527cfa4d5881dbc334ba46fb5f",
            "placeholder": "​",
            "style": "IPY_MODEL_5aae518c17f74e0d99d4c9fea6c12ac8",
            "value": "model.safetensors: 100%"
          }
        },
        "a21083721480483488f565f885fca559": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad00e43f6e2649d98235a41d52b9d6e6",
              "IPY_MODEL_2325b12f9b3642a79e84c7a2fe0fbcb7",
              "IPY_MODEL_d60ace03e891486d9bb6c999959a789f"
            ],
            "layout": "IPY_MODEL_ec00d40b71a0470895e92e31ce4afa23"
          }
        },
        "a2c611f0410444ec87f44c12c936f771": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a31c31bf9c5f4d989a0237579cfaefc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3a37ca5b8dd482c96def0f86b9b7c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cfba888d6ef4ccf954e726985e2ea02",
              "IPY_MODEL_60d8ba0a0f184c8a9991afa3735f2964",
              "IPY_MODEL_e61e147c7dda4c079ae98ccaf47c92ce"
            ],
            "layout": "IPY_MODEL_c2247ac6e48348fe831550619ad75e61"
          }
        },
        "ab2476bfa2ea436bb6e9180006f4184b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac927a60b561417bbbb269d97a6302a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad00e43f6e2649d98235a41d52b9d6e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82724547960d4a1e84d5dae382bdf333",
            "placeholder": "​",
            "style": "IPY_MODEL_d52b9631f3604f64beea14db10cfe56f",
            "value": "modules.json: 100%"
          }
        },
        "b537a4527cfa4d5881dbc334ba46fb5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bccf35b79efd423b8aa97eba9a861c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0da7808b2de74d4095e3563ccc69146d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b9c955172474da99e44117545c8eead",
            "value": 1
          }
        },
        "c0868f77d3b64e5ebaa282cedc9e7e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95d9a8e446724b3fbe3d1f0d57cc76f8",
            "placeholder": "​",
            "style": "IPY_MODEL_eb310f39e56d441cb869f94da2c066ad",
            "value": " 10.5k/? [00:00&lt;00:00, 823kB/s]"
          }
        },
        "c0ba8e7776a04d0c890f2ff74425c3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3ea8df0b88c431ab2d1f9b5af75d2f0",
              "IPY_MODEL_986309e7eef641359bf308efc2e02e73",
              "IPY_MODEL_c922509777154d4eb74f56bfe8e14f79"
            ],
            "layout": "IPY_MODEL_d4a2fc8a502b4525a1f0fe64c91f4a07"
          }
        },
        "c2247ac6e48348fe831550619ad75e61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c290ec6f2d4a4ad382ff965aa5bb4776": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ea8df0b88c431ab2d1f9b5af75d2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24617d2ef7db4b2287b7d2499c1f1d3c",
            "placeholder": "​",
            "style": "IPY_MODEL_a31c31bf9c5f4d989a0237579cfaefc2",
            "value": "config.json: 100%"
          }
        },
        "c66f0e35a5154a72b211fe48129c119c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6d25e74b1b64f1d9451e2bc525f664e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11d3647662044abfa6b85d008840e6b3",
            "placeholder": "​",
            "style": "IPY_MODEL_3302d3f7ad24496ebc12320065a75fce",
            "value": "README.md: "
          }
        },
        "c70865e13b9e4ae7b6f7e6483370d63a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42f44c1c9e554f2eab4736e4bdf1679e",
            "placeholder": "​",
            "style": "IPY_MODEL_420fd067d39246a7864e1577f57cf7c6",
            "value": "tokenizer.json: "
          }
        },
        "c922509777154d4eb74f56bfe8e14f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6341f89b697d4487b5233fff40665646",
            "placeholder": "​",
            "style": "IPY_MODEL_40964214771344d8847716971d010c03",
            "value": " 190/190 [00:00&lt;00:00, 14.4kB/s]"
          }
        },
        "cc03d2aea93a4e4683589e095ccb8bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05426a7b77cf40d4a510daf289d14f21",
            "placeholder": "​",
            "style": "IPY_MODEL_7116c342fa694d9dabeb9af5bd8e3c62",
            "value": " 232k/? [00:00&lt;00:00, 6.14MB/s]"
          }
        },
        "cc410112297f4e03a3748e917fa5414b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc7bf8f29acc450995be812ef7daf0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b83d957fe5b43d298d047bb66a03fe6",
              "IPY_MODEL_29b52c434a2443c3bfa16ec7a5477c8f",
              "IPY_MODEL_80d81dd26f374b9f89ed4ad1277bed30"
            ],
            "layout": "IPY_MODEL_28772262c8884f9a9d3cba914726c1bf"
          }
        },
        "ce4974854ad9496586e7d4f488cc3963": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0708e7d586b4a909460a7212356ffad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03edc535a89745a4ba493688b35293b9",
              "IPY_MODEL_24282af6e82044808c051dc0fcc8e53f",
              "IPY_MODEL_e93c467cd70e4757a78fd381d059ad9f"
            ],
            "layout": "IPY_MODEL_3cb5ad54902d4ca297c0277d72ce0ef3"
          }
        },
        "d4a2fc8a502b4525a1f0fe64c91f4a07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52b9631f3604f64beea14db10cfe56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d60ace03e891486d9bb6c999959a789f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c290ec6f2d4a4ad382ff965aa5bb4776",
            "placeholder": "​",
            "style": "IPY_MODEL_ac927a60b561417bbbb269d97a6302a4",
            "value": " 349/349 [00:00&lt;00:00, 20.3kB/s]"
          }
        },
        "d62308f48db4431cabc8fe1267508e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_352d43ea54d2493e9dae65b495af84b3",
              "IPY_MODEL_bccf35b79efd423b8aa97eba9a861c63",
              "IPY_MODEL_cc03d2aea93a4e4683589e095ccb8bb3"
            ],
            "layout": "IPY_MODEL_a2c611f0410444ec87f44c12c936f771"
          }
        },
        "d74d1b2011eb4551ade473cb19bec260": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a0f0c9f4cd49d9a87cd83c122b61f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59b29cf33710445daea2225549acef25",
              "IPY_MODEL_3f05d915901044209a1b9a711194085a",
              "IPY_MODEL_7dc1bb64184a466f94ba9ec5d9e147dc"
            ],
            "layout": "IPY_MODEL_df80a20cd2fd4c459cc10691779919c6"
          }
        },
        "dbedacda1e114a2cab2622a62e94b0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df101f4dd5d44990a9bf7442f3074297": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_489e699079894c76b298511e39b0f1b3",
              "IPY_MODEL_353b6940c986476597f101deda89944c",
              "IPY_MODEL_47d81250a9d6497bad7d06a1a5eedb1b"
            ],
            "layout": "IPY_MODEL_6a4268995e2440af900218a5256d8a08"
          }
        },
        "df80a20cd2fd4c459cc10691779919c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e61e147c7dda4c079ae98ccaf47c92ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5d69a3616a14eb98db071bf9b584bb2",
            "placeholder": "​",
            "style": "IPY_MODEL_819d655d3b0247008177a12ce58e396f",
            "value": " 112/112 [00:00&lt;00:00, 9.41kB/s]"
          }
        },
        "e7926ad4f10f44bcb8118252d853d2fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e93c467cd70e4757a78fd381d059ad9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb89b2760e304964b7b20029606058b3",
            "placeholder": "​",
            "style": "IPY_MODEL_92e913f6aa684944a6875c1be80adf01",
            "value": " 116/116 [00:00&lt;00:00, 8.71kB/s]"
          }
        },
        "eb310f39e56d441cb869f94da2c066ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec00d40b71a0470895e92e31ce4afa23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1b422ae12d645e89bb9262031812521": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5d69a3616a14eb98db071bf9b584bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f72d94a079e64da2924bd2aa8a8449c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7585d88a77741ebbfb56352a2108fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ef14d70d874bb79922c00e93c85d05",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_488271fe92c844968b50ada324ed9a60",
            "value": 90868376
          }
        },
        "fa28e2bdd58142d999ff504178474a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c70865e13b9e4ae7b6f7e6483370d63a",
              "IPY_MODEL_894117d55abb424ba81341c09396381f",
              "IPY_MODEL_03d1ef3abb5c4547bb640a6a2d263e5f"
            ],
            "layout": "IPY_MODEL_9bcff9030a4e414e9ff5b04276d36d30"
          }
        },
        "fb89b2760e304964b7b20029606058b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fccce84947c142b79d02954394b03e06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd6118cf767a437db9bb91584ba1e294": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fecc150d3a2d4f7184ee1deb79c0c569": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fef9cc760c2c40069d71b5bfa5dd5443": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
